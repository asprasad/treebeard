* Free GPU buffers
    -- Need to generate cleanup methods?
    -- Free result and data buffers allocated in predict_function
* Fix clang build

* There is a chance that all the current changes have broken the runtime and python API
    - UseSparseRepresentation was getting set in JSONReader, which was called directly from the inferenceRunner constructor before. Now this won't happen!
    - Construction of the Serializer (which is passed to InferenceRunner) depends on setting this bool correctly!

* GPU Support
    - [Done] Use schedule to specify which loops correspond to which GPU constructs (grid, threadblock etc)
    - [Done] [Q] If we want to use a common mid-level IR, how do we convey to the final lowering which loops correspond to which GPU constructs?
        - Should we just assume the outer 3 loops are for grid, the next 3 loops are for thread blocks? We could insert dummy loops if user doesn't specify everything.
        - Use attributes on the loops (setAttr)
    - [Done] Lowering from mid-level to low-level needs to take into account the fact that we are compiling to GPUs
        - [Done] Add a new representation for GPU. We need further refactoring of this part of the code!
            - How do we insert the model memrefs?
                - [Option 1]
                    - The init function creates the required memrefs and returns them to the host code
                    - These memrefs are passed to the prediction function
                - [Option 2] ??
                - We can't use globals because they can't be resized
            
            - Each Init function will return a GPU memref that it allocates and initializes in device memory
            - The PredictForest method will take additional parameters (all the memrefs that were globals earlier)
            - Most of the lowering part should be the same. 

    - Reorg forest representation
        - [Done] Remove the assumption that every tree is represented by a single memref
        - [Done] Remove the uses of TiledNumericalNodeType in shared code
        - [Done] Make the lowering of low level ops like LoadTileThreshold etc something specified by the representation
        - [Done] We need more information while lowering LoadTileThresholds/Indices
            - [Preferred - Opt 1] Add the tree value as an argument to these ops
                - Replace the tree value by the tree index (current representations replace it with the treeMemref)
            - [Opt 2] Provide some mechanism to generate index computation code required
                - This won't work when we need the tree for some other reason!
        - [Done] New representation for reorg forest
            - Generate Globals will generate different Init methods for each model memref (thresholds, indices)
                - This rep will only support scalar code gen 
                - 
            - Need to stop using the JSONReader class
                - Everything that is being written by the JSONReader class needs to go through the serializer 
                    - batch size
                    - row size
                    - 
                - Uses of the class
                    - ForestCreator
                    - InferenceRunner
        - [Done] How do we construct the right serializer? Should we have a global map?
    
    - Shared memory support (for trees)
        - [Done] Add caching to schedule
        - [Done] New op to model caching of trees
        - [Done] Type support to represent ensemble subsets (that are cached)
        - Insert cache op in the right loops
            - Refactor loop code generator to go through common choke point while constructing loops
                - How will caching be generated for GPU dimensions (since we're constructing a single par loop for all these indices)
                - [Done] Ask Sampath about the pipelined loops
        - First step is to lower cache op to do nothing -- just correctly interpret the ensemble subset type 
        - Lowering cacheTrees
            - How do we compute the size of the shared memory buffer?
                - Since this lowering is done with the lowering of the forest to memrefs, we can compute sizes
            - How do we represent trees in shared memory? Needs to come from representation
                - First add globals when going from MIR -> LIR 
                - While lowering from LIR -> LLVM, after GPULaunchOp outlining, we can run a custom pass to lower the globals to shared mem buffers
                    - Which globals correspond to shared mem buffers can come from the representation
            - How do we read in to shared memory across threads?
                - Extent to be read from model buffers has to be computed each time.
                - The representation decides the strategy to read the values in to shared memory
            - 

* Add a Treebeard context to store all configuration
    - Update the python API!
* Make interfaces for mem rep and code gen extensible
    - LowerEnsembleToMemrefs 
        - Object that abstracts 
            - Serialization of the model
            - what globals to insert
            - How to initialize these
            - All the specific code generation 
                - GetLeafValueOpLowering
                - GetTreeOpLowering              
                - IsLeafOpLowering
                - GetTreeClassIdOpLowering
                    - Needs to completely offload code gen to object
                - GetRootOpLowering
                    - No change?
                - InterleavedTraverseTreeTileOpLowering
                - TraverseTreeTileOpLowering
                    - Moving to children (TraverseTile)
    - LowerToLLVM
        - Type translations
        - Lowering for Load*
* Unify the serialization of probability based tiling and the normal sparse tiling        
* Make interfaces for mem rep and code gen extensible
* Make extensibility a concept in scheduling lang
* Make the command line interface take a JSON for the configuration (so scripts don't break everytime)
* Schedule exploration
* Pipelining for general case
* SIMD for multiple walks
* AMD specific opts -- gather etc
* GPU code gen
* Loop Parallelization -- Support more non-trivial forms of parallelization. Can we do some experiments to figure out if there are benefits?
* Loop Tiling
* LightGBM
* Document compile times

Python API
    - [Done] Expose reordering, pipeling, parallelism, stats CSV path
    - [Done] Expose common schedules
    - Expose PeelProbabilisticWalk
Fix the tiling for parallelization

MAJOR HOLES
1. LightGBM support, random forest support
2. Categorical features
3. Parallelization 
4. Exploring non-trivial schedules
5. Not interleaving anything other than across inputs

QUESTIONS
1. Why aren't we getting performance with smaller tile sizes? Will a tile size of 16 help?
2. Pipelining with prob tiling
    - Can we unroll fully and pad until we cover 90% (or some number) of inputs? And then maybe pad all other subtrees to be the same depth?

    Code would look like this  (assuming first stage is 2 steps):
    n1 = TraverseTile(n1, t, x1)
    n2 = TraverseTile(n2, t, x2)
    n1 = TraverseTile(n1, t, x1)
    n2 = TraverseTile(n2, t, x2)
    
    n1' = IsLeaf(n1)*Dummy + !IsLeaf(n1)*n1
    n2' = IsLeaf(n2)*Dummy + !IsLeaf(n2)*n2
    while (!IsLeaf(n1) || !IsLeaf(n2)) {
        n1' = TraverseTile(n1', t, x1)
        n2' = TraverseTile(n2', t, x2)
    }
    n1 = IsLeaf(n1)*n1 + !IsLeaf(n1)*n1'
    n2 = IsLeaf(n2)*n1 + !IsLeaf(n2)*n2'
--------------------------------------
    n1 = TraverseTile(n1, t, x1)
    ...
    nk = TraverseTile(nk, t, xk)
    ...
    ...
    n1 = TraverseTile(n1, t, x1)
    ...
    nk = TraverseTile(nk, t, xk)
    
    while (!IsLeaf(n1) ||... !IsLeaf(nk)) { // If any one is not a leaf
        n1' = IsLeaf(n1)*Dummy + !IsLeaf(n1)*n1
        ...
        nk' = IsLeaf(nk)*Dummy + !IsLeaf(nk)*nk
        while (!IsLeaf(n1) && ... !IsLeaf(nk)) { // If any of them is a leaf
            n1' = TraverseTile(n1', t, x1)
            ...
            nk' = TraverseTile(nk', t, xk)
        }
        n1 = IsLeaf(n1)*n1 + !IsLeaf(n1)*n1'
        ...
        nk = IsLeaf(nk)*n1 + !IsLeaf(nk)*n2'
    }

3. Can the memory representation also be abstracted into a scheduling language type of thing?

- Locality Optimizations
    - Decide the tiling of the (batch, forest) iteration space and make it a property of the predictForest op
    - Properties
        - How many trees to travese at a time?
        - How many rows to traverse per tree?
        - Order in which to traverse trees
            - Which trees need dedicated code and which trees share code
        - Which trees to pipeline
        - Which trees to SIMDize

Scheduling Language
    - Schedule class
        - [Done] Lineage tree
        - [Done] Nesting tree
        - [Done] Reorder
        - [Done] Tile
        - Split -- duplication of nested loops causes problems
        - [Done] Flags for optimizations on the IndexVariable class
            - Unroll, Simdize, Pipeline, Parallelize
            - Expand, PeelWalk, Prefetch
    - Schedule attribute
        - [Done] MLIR attribute classes for schedule
        - [Done] Make them part of the Predict forest op
        - [Done] Expose APIs from JSONParser to construct and modify schedule object
    - Lowering using the schedule
        - [Done] Tiling
        - [Done] Unrolling
        - [Done] Reordering
        - [Done] Splitting
        - Pipelining
        - Unroll walk
        - Parallel
        - Expand
        - Simdize
    - How do we use schedules from the command line?

XGBoost comparison
    - [Done] Write JSON file from ForestJSONReader
    - [Done] Read the JSON file from the execution helper
    - [Done] Change SO APIs to take value JSON filename and use that for execution
    - [Done] Build a separate SO for execution part
    - Python wrappers
        - [Done] Write python tests for correctness (same as runtime tests, but on real data when possible)
        - [Done] For benchmarking, write runtime method that iterates over a large dataset in batches (for fair comparison with Xgboost)
        - [Done] Write a python wrapper to run benchmarks (on real data)
        - Inference method -- Remove the assumption that inputs and outputs are float type
            - Persist input and return types in the JSON
        - Expose JIT through Python API

[Done] Merge multiclass support
    - [Done[ Add tests on actual inputs for covtype
    - [Done] Add code to xgboostbenchmarks to benchmark covtype
    - [Done] Modify code in compileutils.cpp to be able to handle multi-class
[Defer] Fix generated code to work with nans
[Done] Profile and find reasons why random inputs are faster than test inputs
    - Seems to be caching problems and some increase in branch misprediction rate
Add sparse support for mutliclass
    - Add serialization of classes
    - Add new globals if multiclass
    - Lowering for GetClassID (should be the same)
Figure out tree data structures to reduce footprint when there is severe imbalance (due to probabilistic tiling)
    - Move all leaves to the leaf array for sparse rep by adding one more hope for "internal" leaves
    - Add bitmask to indicate which children are leaves 

[Done] Add covtype to runtime tests
[Done] Add covtype to xgboost benchmarks
Finish the microbenchmarking for gather
LightGBM support

Tiling optimization (DP to minimize skew?)
    - [Done] Dump profiling output to a file or pass profiling input CSV as an option to the compiler
    - [Done] Compute edge probabilities from leaf counts
    - [Done] Tile by probabilities
    - How do we write tests?
    - [Done] Add covtype to tests and benchmarks
    - Change heuristic to only aggressively tile skewed subtrees
    - Profiling/Analysis
        - [Done] Does instruction count reduce?
        - [Done] How many trees are being aggressively tiled?
        - [Done] Tile shape stats (max, min, median, mean for number of tile shapes)
        - [Done] How many tile shapes are being added by the tile padding?
        - [Done] Profile generated SOs with vtune
        - How many tile shapes do we really need to add?
            - Are partial tiles subsets of any of the full tiles in the tree?

- Pad tiled trees so all leaves are at the same depth
    - Should we implement a DP to minimize max depth?

- Remove the extra hop (new representation)
    - Serialization changes to 
        1. [Done] Serialize the extra fields
        2. [Done] Add them to JSON
    - [Done] (LowerSparseEnsemble) Add lowering to use two indices and pick one
        - [Done] Changes are only needed for TraverseTile
        - [Done] No changes are needed to IsLeaf and GetLeafValue
    - [Done] Initialization of model memref
        - Change the init method to take the extra fields
        - New op to initialize the sparse tile (with the extra index)
    - [Done] Reverse the direction of bits in the leaf bit mask
    - [Done] Changes to ExecutionHelper to call the right init method
    - [Done] LLVM type conversion
    - Pad single node trees
    - 

Unrolling Treewalk 
Pipeline
    - Reuse tiles in registers if possible (root tile)
    - How do we manage probabilistic tiling?
        - Partial probabilistic (like until 90% inputs are covered) and partial uniform?
Expand tree walk
Try tile size of 16
LightGBM support
Comparison with TreeLite

Parallelization
Schedule exploration
SIMD across trees
Reorder trees to exploit feature set commonality
Store features in registers 

Code gen for unequal tile size across trees
    - Serialization when tile sizes are unequal across trees
    - Generate >1 model memrefs
    - ??
    - Make the look-up in ForestJSONReader based on the name of a global rather than the tile size etc. We're unnecessarily constraining the IR to only have a single ensemble constant with a particular (tile size, threshold type, feature index type) combination now. Instead we should just generate a unique global name for each ensemble constant.

*[Done] Generate method to initialize model memref that takes buffers containing actual values as arguments
    - This will be safer than guessing the layout of the tile struct (since we'll pass the thresholds and indices as separate arrays)

* [Done] Memref of memrefs, Memref of struct types
* [Done] Initialization of the model memrefs
    - Method to figure out the storage needed for each tree (number of nodes, size of each node)
    - Generate constants that can be used to initialize the model memref
* Add SIMD and Pipeline Containers
    - How will an Op contain other Ops?!
* Type inference and checking passes (NodeType and TreeType)
* [Done] Verify whether the features of the input are indexed with base 1 or 0 in XGBoost
* [Done] Tree types need to be part of the ensemble type (we may need to be able to tell tree tiling for a certain tree for example)

* [Defer] MLIR parse routines

* [Done] Add tiling information to tree types
    - Tiling info needs to be in two places -- the DecisionTree class and the TreeType class
* [Done] [Bug] The tensor from which result[i] is being read is not the "iteration tensor" of the the tree loop. 
* [Done] TreeEnsemble attribute
    - Needs to take the TreeEnsemble object and construct a storage class and attribute object
    - Hash for the storage key needs to be figured out (Type, TreeEnsemble)
* [Done] API to build TreeEnsembleAttribute from JSON parser
* [Done] Calls to Builder interface from JSON parser
    - Build methods for the predict op
    - Only needs to build two ops and set the corresponding attributes
* [Done] MLIR print routines for dialect
* [Done] Add batch size to high level IR 
* [Done] Tree constant op
* [Done] Forest constant op
* [Done] Figure out how to implement node types (sub-typing)
* [Done] Add document to describe the next level IR
* [Done] Implement the tree attribute
* [Done] Add TraverseTile for constant tree

CodeGen for tile size == 1 for all trees
    * [Done] Add threshold and feature index type to tree type
    * [Done] Add tree types to forest type (list of individual tree types)
    * [Done] Write tiling helpers
        - [Done] Serialize model into file
        - [Done] Read model from file into buffer
     
    * [Done] LoadOp for tiled trees stored as memrefs
    * [Done] BUG The length of each tree needs to be passed to the memref subview op
    * [Done] Write pass to replace node values flowing through the IR with index values
        - Remove the NodeToIndex and IndexToNode ops
    * [Done] Lower all other dialects to LLVM
    * [Done] Lower custom tree ops to LLVM dialect
        - Non vector implementation
    * [Done] Initialization routines for length and offset memrefs need to be implemented
    * [Done] Seperate out the tests and write some basic test infrastructure
    * [Done] Write a set of tests that generate small forests and test generated code against that
    * [Done] Write some basic debugging infrastructure for JIT'ed code
    * [Done] Tests with batch size > 1
    * [Done] Compare results with XGBoost for small models
        * [Done] Write a script to generate test cases that have the following : 
            - JSON file
            - Input and output CSV
        * [Done] Write tests in C++ that read these, compile and check results
    * [Done] Compare results with XGBoost for full models


CodeGen for tile size != 1
    - Serialization when tile sizes are not 1
        - [Done] Verify that tiling is valid 
            - Leaves not part of any tile
            - Each tile has exactly one entry point
            - Tiles are connected
            - All tiles except leaves have equal size
        - [Done] Implementations in class DecisionTree to serialize thresholds and indices
            - [Done] Construct a tree of the tiles
        - [Done] Tile support in buffer initialization routines (ForestJSONReader)
        - [Done] Write the tileID with each tile's value
    - Change codegen 
        - [Done] Ensemble to Memref Lowering
            - Generate buffers to hold the lookup tables
            - Initialization routines for the lookup tables
            - Compact comparison outcomes into an integer to use as a lookup index
            - Generate lookups rather than computing next tile indices using (2*i + x)
        - [Done] LLVM Lowering
            - Vector implementations of tree Load* ops
        - [Done] [Bug] Fix failing Load* tests (Maybe just use structs with natural alignment now?)
        - [Done] Write tests for the new Initialization method
        - [Done] Write a general tiling pass to tile the mid-level IR
    - Utils
        - [Done] Number of tiles of a given size
        - [Done] Generate lookup tables for child tile indices given tile comparison outcomes

Performance Tuning for Tiled Code
    - [Done] Add XGBoost tests with different feature index types
    - Add a "child" pointer to each tile (improve leaf duplication, empty space)
        - [Done] Change TiledTreeNodeType to include type for the child index and sparsity
        - [Done] Return sparse representation from decision tree and tiled tree
        - [Done] Store sparse trees in ForestJSONReader (model, offsets, lengths)
        - [Done] Check that existing initialization works for length and offset of model array
            - Seems to work because these values are computed based on "numberOfTiles" in trees and this shouldn't change with sparse. 
        - [Done] Initialization for leaves array (and its offset and length array)
        - Lower ensemble to memref
            - [Done] Add leaves memref, offset and length arrays for leaves memref
            - [Done] Change the init method to initialize the child 
            - [Done] Change IsLeaf, TraverseTreeTile, EnsembleConstant, GetLeafValue (only the index computation part)
            - No changes to  GetThreshold, GetFeatureIndices, Comparison, LUT lookup, LUT computation,  GetTree, GetRoot (as long as struct has thresholds in the beginning etc)
            - [Defer] New op to simultaneously read both child index and tile shape?
        - [Done] LLVM lowering (new op, type conversion)

    - [Done] Store leaves in a "different" array so leaf values don't have to be duplicated as full tiles

[Done] SO support for sparse representation

Remove Extra Hop - Attempt 2
1. Serialization changes 
    - All leaves upto a certain level into the tiles array
    - Add extra hops if needed for any leaves after that
2. Tiling changes
    - Add whether tiled tree was probabilistically tiled
2. Code gen changes
    - New op "Peeled tree walk"
    - Add to schedule
    - Set unroll factor on the walk tree op

ISSUES
1. [Solved] How do we know which loop is the batch loop(s) and which loop is the tree loop(s) in the mid-level IR? For example, say we want to tile, we've already lost the information about which loop we're dealing with -- we'll have to infer it from what the loop index is being used for!