# Scheduling Language
The goal of the scheduling language is to express the following
* The order in which the iteration space over the batch of inputs (batch) and trees in the forest (tree) is to be traversed. Note that there is a reduction over the tree dimension.
* Intra or inter tree optimizations that are to be performed on a tree or set of trees (tree walk unrolling, pipelining, SIMDize etc).

The reasons to use a scheduling language rather than a hard-coded lowering are as follows
* Making the scheduling specification external to the compiler allows us to 
more easily build auto-schedulers and auto-tuners.
* It is very hard to come up with a template loop nest that works for all models (for example, tree sizes may vary across the model making it necessary to iterate different number of trees at different times). 
* A scheduling language will make writing newer locality optimizations faster since no changes to the compiler infrastructure will be needed.
* Adding support for additional hardware targets (GPUs, FPGAs), will be much easier with a scheduling language.

There are some simplifying assumptions and limitations in the current design
* Tree traversals are considered atomic. There is no way to express partial tree traversals or schedule individual node/level computations. 
* Accumulation of tree predictions is done immediately (as opposed to, for example, collecting all predictions and performing a reduction later).

## Language Definition
We broadly have three classes of directives in the language. The first is a set of loop nest modifiers that are used to specify the structure of the loop nest to walk the iteration space. The second is a set of clauses that specify intra and inter tree optimizations. Finally, we have a class of attributes that control how reductions are performed.

### Loop Modifiers
There are two special index variables -- ```batch``` and ```tree```. The clauses modify these index variables or index variables derived from these (through the application of clauses).
* **tile**: Tile the passed index variable using a fixed tile size.
* **split**: Split the range of the passed index variable into two parts. The range of the first part is specified by an argument.
*  **unroll**: Unroll an index completely
*  **reorder**: Reorder the specified indices. The specified indices must be successive indices in the current loop nest.
* **specialize**: Generate separate code for each iteration of the specified index variable. This is useful while parallelizing across trees and these trees have 
different depths.

**TODO Add GPU constructs.**

The default loop order (as currently generated by the compiler) is (batch, tree), i.e, for each row in the input batch, go over all trees.

The following are examples of how the loop modifiers can be used.
* The loop order used by xgboost is (tree, batch) -- walk one tree for all inputs in the batch before moving to the next tree. The corresponding schedule would be
  ```C++
  reorder(tree, batch)
  ```
* The below schedule computes 2 trees at a time over the whole batch.
  ```C++
  tile(tree, t0, t1, 2)
  reorder(t0, batch, t1)
  ```
* If we additionally only want to compute over 4 input rows (rather than the whole batch) for every 2 tree, and then move onto the next 2 trees for the same set of inputs, then the schedule is as follows. 
  ```C++
  tile(batch, b0, b1, 4) 
  tile(tree, t0, t1, 2)
  reorder(b0, t0, b1, t1) 
  ```
### Optimizations
The following clauses provide ways to optimize the inference routine being generated.
* **cache**: Cache the working set of one iteration of the specified loop corresponding
to this index. This can be specified on either batch or tree loops. Specifying it 
on a batch loop leads to all rows accessed in a single iteration of the loop 
being cached. Similarly, specifying it on a tree loop leads to all trees accessed in 
one iteration of that loop being cached.
* **parallel**: Execute the loop corresponding to this index in parallel.
* **interleave**: Interleave the execution of the tree walks within the current index (must be applied on an inner most index).
* **unrollWalk**: Unroll tree walks at the current index. 
* **peelWalk**: Peel the first n steps of the specified tree walk and don't check for leaves for that number of steps.

### Reduction Optimization
* **atomicReduce**: Use atomic memory operations to accumulate values across 
parallel iterations of the specified loop. 
* **sharedReduce**: Only applies to GPU compilation. Specifies that intermediate
results are to be stored in shared memory.
* **vectorReduce**: Use vector instructions with the specified vector width 
to reduce intermediate values across parallel iterations of the specified loop.

**TODO Add examples for RAPIDS, Tahoe (Maybe show some strategies can be encoded?)**