\section{Motivation}

In this section, we first motivate the need for a scheduling language 
by showing how a model can be compiled in different ways using the example
in Figure \ref{Fig:HIRExample} and subsequently, we show how drastically 
performance can vary across these variants for real benchmarks.

First, we consider the schedule that processes one tree at a time 
for all input rows and unrolls all tree walks. 
The schedule splits the loop over the trees into two loops -- one that
iterates over the first two trees (Trees 1 and 2 with depth 1) and 
the second that iterates over the last two trees (Trees 3 and 4 with
depth 2). The schedule then unrolls the tree walks for each tree.
\begin{lstlisting}[style=c++]
  reorder(tree, batch)  
  split(tree, t0, t1, 2)
  unrollWalk(t0, 1)
  unrollWalk(t1, 2)
\end{lstlisting}
The concrete implementation of this schedule (in one of \Treebeard{}'s IRs) 
is as follows.
\begin{lstlisting}[style=c++]
  model = ensemble(...)
  for t0 = 0 to 2 step 1 {
    T = getTree(ensemble, t0)
    for batch = 0 to BATCH_SIZE step 1 {
      treePred = walkDecisionTree(T, 
                    input[batch]) <unrollDepth = 1>
      reduce(result[batch], treePred)
    }
  }
  for t1 = 2 to 4 step 1 {
    T = getTree(ensemble, t0)
    for batch = 0 to BATCH_SIZE step 1 {
      treePred = walkDecisionTree(T,
                    input[batch]) <unrollDepth = 2>
      reduce(result[batch], treePred) <'+', 0.0>
    }
  }
\end{lstlisting}
This schedule is ideally suited for a single-core CPU. It maximizes 
the reuse of trees in the L1 cache and also minimizes the amount of
branching by unrolling tree walks. However, it doesn't exploit  
any parallelism and is therefore ill-suited for parallel processors.

One form of parallelism that can be exploited is to process rows in 
parallel. However, with massively parallel processors like GPUs,
this may not yield sufficient parallel work. Another option is to also 
parallelize across trees. A possible schedule to accomplish this is as follows.
\begin{lstlisting}[style=c++]
  tile(tree, t0, t1, 2)
  reorder(batch, t1, t0)
  split(t0, t0_1, t0_2, 2)
  unrollWalk(t0_1, 1)
  unrollWalk(t0_2, 2)
  gpuDimension(batch, grid.x)
  gpuDimension(t1, block.x)
\end{lstlisting}
This schedule generates an inference function that runs on the GPU. 
The inference routine processes one input row per thread block (since the \op{batch}
loop is mapped directly to \op{grid.x}).
It also splits the trees into two sets by tiling the \op{tree} loop.
Each of the two sets is processed in parallel. We unroll the tree walks 
for each tree. The IR generated is as follows. 
\begin{lstlisting}[style=c++]
  model = ensemble(...)
  par.for batch = 0 to BATCH_SIZE step 1 <grid.x> {
    par.for t1 = 0 to 2 step 1 <block.x> {
      for t0_1 = 0 to 2 step 2 {
        T = getTree(ensemble, t0_1 + t1)
        treePred = walkDecisionTree(T, 
                        input[batch]) <unrollDepth = 1>
        reduce(result[batch], treePred)
      }
      for t0_2 = 2 to 4 step 2 {
        T = getTree(ensemble, t0_2 + t1)
        treePred = walkDecisionTree(T,
                        input[batch]) <unrollDepth = 2>
        reduce(result[batch], treePred) <'+', 0.0>
      }
    }
  }
\end{lstlisting}

In the case of this schedule, the \op{reduce}
operation needs special consideration. In order to correctly generate 
code for this schedule, the compiler needs to determine that parallel 
iterations of the \op{t1} 
loop accumulate into the same element of the \op{result} array.
One possible solution is to rewrite the reduction so that each parallel 
iteration accumulates into a different array element by introducing 
a temporary buffer (\op{tempResults}) as follows.
\begin{lstlisting}[style=c++]
  float tempResults[2][BATCH_SIZE]
  model = ensemble(...)
  par.for batch = 0 to BATCH_SIZE step 1 <grid.x> {
    par.for t1 = 0 to 2 step 1 <block.x> {
      for t0_1 = 0 to 2 step 2 {
        T = getTree(ensemble, t0_1 + t1)
        treePred = walkDecisionTree(T, 
                      input[batch]) <unrollDepth = 1>
        reduce(tempResults[t1][batch], treePred)
      }
      for t0_2 = 2 to 4 step 2 {
        T = getTree(ensemble, t0_2 + t1)
        treePred = walkDecisionTree(T,
                      input[batch]) <unrollDepth = 2>
        reduce(tempResults[t1][batch], treePred) <'+', 0.0>
      }
    }
    result[batch] = reduce_dimension(tempResults[:][batch], 0)
  }
\end{lstlisting}
Here, partial results are accumulated into \op{tempResults} and then
reduced across the \op{t1} dimension (represented by the \op{reduce\_dimension}
operation) to get the final result.

As is evident from these examples, it is possible to optimize 
the inference routine in different ways. Also, the structure of the loop 
nest in the inference routine can get quite complex even 
for simple schedules. Writing these routines by hand 
is error-prone and time-consuming.  We believe that designing a 
scheduling language to encapsulate these strategies and a principled code 
generator to automatically generate code based on the schedule is the 
best approach. 

We find that these different strategies can have significantly different
performance. Figure \ref{fig:sensitivity} shows the\dots.

Several problems need to be solved in order to build the kind of 
schedule guided retargetable compiler required. We had to enable the
compiler to represent and optimize reductions, deal uniformly with different
in-memory representations of the model, design optimizations to effectively 
use the memory hierarchy of the target processor (shared memory on GPUs and 
cache on the CPU) and finally be able to generate target specific code. 
The rest of the paper describes these challenges in detail and how we
solved them in \Treebeard{}.

% Our main aim while designing \Treebeard{} was to unify the diverse set of implementation
% strategies that have been used in existing systems for decision tree inference. Some 
% differences in these systems are as follows:
% \begin{itemize}
%   \item Decision tree inference is run on several platforms including CPUs and GPUs. The 
%   implementations used on each of these platforms are different and the techniques used
%   to optimize them are also different.
%   \item A diverse set techniques have been proposed for optimization of decision tree 
%   inference on CPUs and GPUs \cite{VPred, Tahoe, Treelite, XGBoost, Hummingbird, QuickScorer, FIL}. 
%   No system exists that unifies the disparate optimizations implemented in these systems.
%   \item A very extensive design space of optimizations exists for decision tree inference
%   outside the few that have been proposed in the literature. However, currently no 
%   system exists that is capable of exploring this space and identifying the best set 
%   of parameters to use for a given model and platform.
%   \item Different systems use different in-memory representations for the model. For example,
%   XGBoost uses a sparse representation, RAPIDs FIL uses what is called the reorg representation 
%   and Tahoe uses a variation of the reorg representation. Currently, systems implement 
%   inference kernels that are tied to a single representation of the model. Again, this means
%   that no current system can explore different combinations of in-memory representations 
%   and optimizations.
% \end{itemize}
% At high-level, to make \Treebeard{} capable of unifying these differences, we design 1)  
% expressive intermediate representations that can represent and compose several proposed 
% optimizations 2) a scheduling language that specifies the structure of the
% generated code and 3) a plugin mechanism with which different in-memory representations
% can be composed with different optimizations. Finally, we develop a heuristic to
% explore the extensive optimization space that \Treebeard{}'s design enables.

% While a diverse set techniques have been proposed for optimization of decision tree 
% inference on CPUs and GPUs \cite{VPred, Tahoe, Treelite, XGBoost, Hummingbird, QuickScorer, FIL},
% a very extensive design space of optimizations exists 
% outside what has been proposed in the literature. Furthermore, decision tree inference 
% is run on several platforms including CPUs and GPUs. The implementations used on each of 
% these platforms are different and the techniques used to optimize them are different.
% To make matters even more complicated, several in-memory representations
% have been proposed for decision tree models. For example, XGBoost\cite{XGBoost} uses a sparse representation,
% RAPIDs FIL\cite{FIL} uses what is called the reorg representation and Tahoe uses a variation of the reorg
% representation. 
% \TODO{Can we add some numbers here to show that different models/batch sizes need different optimizations?}

% To solve the problems of exploring the design space of optimizations for decision tree
% inference and enabling portable performance, we build several techniques in \Treebeard{}, 
% an open source compiler infrastructure for decision tree inference. To make \Treebeard{}
% capable of unifying these different techniques and targets, we do the following. 
% \begin{itemize}
%   \item We design a scheduling language that encapsulates various optimization techniques
%   and controls the structure of the generated code.
%   \item We design an MLIR dialect to represent and optimize reductions and use this 
%   dialect within \Treebeard{} to enable the generation of different variants of 
%   inference routines.
%   \item We extend \Treebeard{}'s intermediate representations to include operations like caching.
%   We were able to easily reuse and extend \Treebeard{}'s IR as it was built as an MLIR dialect.
%   \item We design a plugin mechanism with which different in-memory representations
%   can be composed with different optimizations.  
% \end{itemize}