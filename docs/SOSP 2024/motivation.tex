\section{Motivation}

% Our main aim while designing \Treebeard{} was to unify the diverse set of implementation
% strategies that have been used in existing systems for decision tree inference. Some 
% differences in these systems are as follows:
% \begin{itemize}
%   \item Decision tree inference is run on several platforms including CPUs and GPUs. The 
%   implementations used on each of these platforms are different and the techniques used
%   to optimize them are also different.
%   \item A diverse set techniques have been proposed for optimization of decision tree 
%   inference on CPUs and GPUs \cite{VPred, Tahoe, Treelite, XGBoost, Hummingbird, QuickScorer, FIL}. 
%   No system exists that unifies the disparate optimizations implemented in these systems.
%   \item A very extensive design space of optimizations exists for decision tree inference
%   outside the few that have been proposed in the literature. However, currently no 
%   system exists that is capable of exploring this space and identifying the best set 
%   of parameters to use for a given model and platform.
%   \item Different systems use different in-memory representations for the model. For example,
%   XGBoost uses a sparse representation, RAPIDs FIL uses what is called the reorg representation 
%   and Tahoe uses a variation of the reorg representation. Currently, systems implement 
%   inference kernels that are tied to a single representation of the model. Again, this means
%   that no current system can explore different combinations of in-memory representations 
%   and optimizations.
% \end{itemize}
% At high-level, to make \Treebeard{} capable of unifying these differences, we design 1)  
% expressive intermediate representations that can represent and compose several proposed 
% optimizations 2) a scheduling language that specifies the structure of the
% generated code and 3) a plugin mechanism with which different in-memory representations
% can be composed with different optimizations. Finally, we develop a heuristic to
% explore the extensive optimization space that \Treebeard{}'s design enables.

While a diverse set techniques have been proposed for optimization of decision tree 
inference on CPUs and GPUs \cite{VPred, Tahoe, Treelite, XGBoost, Hummingbird, QuickScorer, FIL},
a very extensive design space of optimizations exists 
outside what has been proposed in the literature. Furthermore, decision tree inference 
is run on several platforms including CPUs and GPUs. The implementations used on each of 
these platforms are different and the techniques used to optimize them are different.
To make matters even more complicated, several in-memory representations
have been proposed for decision tree models. For example, XGBoost\cite{XGBoost} uses a sparse representation,
RAPIDs FIL\cite{FIL} uses what is called the reorg representation and Tahoe uses a variation of the reorg
representation. 
\TODO{Can we add some numbers here to show that different models/batch sizes need different optimizations?}

To solve the problems of exploring the design space of optimizations for decision tree
inference and enabling portable performance, we build several techniques in \Treebeard{}, 
an open source compiler infrastructure for decision tree inference. To make \Treebeard{}
capable of unifying these different techniques and targets, we do the following. 
\begin{itemize}
  \item We design a scheduling language that encapsulates various optimization techniques
  and controls the structure of the generated code.
  \item We design an MLIR dialect to represent and optimize reductions and use this 
  dialect within \Treebeard{} to enable the generation of different variants of 
  inference routines.
  \item We extend \Treebeard{}'s intermediate representations to include operations like caching.
  We were able to easily reuse and extend \Treebeard{}'s IR as it was built as an MLIR dialect.
  \item We design a plugin mechanism with which different in-memory representations
  can be composed with different optimizations.  
\end{itemize}