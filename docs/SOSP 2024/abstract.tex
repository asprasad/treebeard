\begin{abstract}
  Decision tree based models are generated by some of the most commonly 
  used machine learning algorithms like gradient boosting and random forests.
  These algorithms are used very widely in practice and the resulting models
  are deployed to production at scale. The models are served on a variety of 
  machines including CPUs and GPUs and inference performance on all these 
  machines is an important consideration. On CPUs, models are served using
  either libraries such as XGBoost, Sklearn, and LightGBM or specialized 
  compilers like \TreebeardOLD{}, Treelite, and Hummingbird. On GPUs, XGBoost 
  and RAPIDs FIL are popular libraries. However, these libraries and compilers
  do not provide portable performance across the targets of interest. They 
  also leave significant optimization opportunities unexplored. 

  In this paper, we present the design of \Treebeard{}, a compiler infrastructure 
  for decision tree based models that significantly expands the design space 
  that can be explored and provides portable performance across CPU and GPU 
  targets. We rearchitect the open-source \Treebeard{} infrastructure 
  and significantly extend it to enable high-performance code generation 
  across target processors. Specifically, we design a scheduling language 
  that encapsulates the large optimization space for decision tree inference
  and develop techniques to explore this space. Further,
  we design techniques to represent and optimize parallel reductions, extend
  \TreebeardOLD{}'s intermediate representations to include operations like caching
  and rearchitect \TreebeardOLD{} to allow in-memory representations to be built 
  as plugins. \TODO{Need to say we investigate how to structure a compiler for 
  decision tree inference that can target multiple hardware platforms}

  We implement \Treebeard{} using the MLIR compiler infrastructure and
  compare the performance of the generated code against state-of-the-art
  systems on CPUs and GPUs. On GPUs we compare performance against XGBoost,
  RAPIDs FIL, and Tahoe. We find that \Treebeard{} is significantly faster
  than other systems that target GPUs across a wide range of batch sizes 
  and models on different NVIDIA GPUs. \Treebeard{} is an order of magnitude faster than XGBoost and
  about 2-3$\times$ faster than RAPIDs FIL and Tahoe on average. \TODO{Make 
  this more precise. But how?}. We also show how \Treebeard{} can generate  
  high-performance code on other GPUs by running the benchmarks on an AMD GPU.
  On CPUs, we compare against \TreebeardOLD{} and show how the  
  optimization opportunities exposed by \Treebeard{}'s scheduling 
  language allow it to generate higher performance code by specializing  
  code for different batch sizes.
\end{abstract}