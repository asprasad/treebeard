\begin{comment}
\begin{abstract}
  Decision tree based models are generated by some of the most commonly 
  used machine learning algorithms like gradient boosting and random forests.
  These algorithms are used very widely in practice and the resulting models
  are deployed to production at scale. The models are served on a variety of 
  machines including CPUs and GPUs and inference performance on all these 
  machines is an important consideration. On CPUs, models are served using
  either libraries such as XGBoost, Sklearn, and LightGBM or specialized 
  compilers like \TreebeardOLD{}, Treelite, and Hummingbird. On GPUs, XGBoost 
  and RAPIDs FIL are popular libraries. However, these libraries and compilers
  do not provide portable performance across the targets of interest. They 
  also leave significant optimization opportunities unexplored. 

  In this paper, we present the design of \Treebeard{}, a compiler infrastructure 
  for decision tree based models that significantly expands the design space 
  that can be explored and provides portable performance across CPU and GPU 
  targets. We rearchitect the open-source \Treebeard{} infrastructure 
  and significantly extend it to enable high-performance code generation 
  across target processors. Specifically, we design a scheduling language 
  that encapsulates the large optimization space for decision tree inference
  and develop techniques to explore this space. Further,
  we design techniques to represent and optimize parallel reductions, extend
  \TreebeardOLD{}'s intermediate representations to include operations like caching
  and rearchitect \TreebeardOLD{} to allow in-memory representations to be built 
  as plugins. \TODO{Need to say we investigate how to structure a compiler for 
  decision tree inference that can target multiple hardware platforms}

  We implement \Treebeard{} using the MLIR compiler infrastructure and
  compare the performance of the generated code against state-of-the-art
  systems on CPUs and GPUs. On GPUs we compare performance against XGBoost,
  RAPIDs FIL, and Tahoe. We find that \Treebeard{} is significantly faster
  than other systems that target GPUs across a wide range of batch sizes 
  and models on different NVIDIA GPUs. \Treebeard{} is an order of magnitude faster than XGBoost and
  about 2-3$\times$ faster than RAPIDs FIL and Tahoe on average. \TODO{Make 
  this more precise. But how?}. We also show how \Treebeard{} can generate  
  high-performance code on other GPUs by running the benchmarks on an AMD GPU.
  On CPUs, we compare against \TreebeardOLD{} and show how the  
  optimization opportunities exposed by \Treebeard{}'s scheduling 
  language allow it to generate higher performance code by specializing  
  code for different batch sizes.
\end{abstract}
\end{comment}
\begin{abstract}
  
  With the rapid proliferation of machine learning, the demand for on-device model inference has surged. Concurrently, the hardware ecosystem is evolving rapidly, leading to an increasing heterogeneity in client machines.
  %While CPUs have been the mainstay for machine learning inference, the availability of GPUs holds promise to scale to bigger and more powerful models. 
  This paper is motivated by the problems encountered when targeting inference of decision tree based models, the most popular models on tabular data, to run at peak performance on diverse client hardware. We evaluated existing solutions and found that they do not provide portable performance across different hardware targets.
  %Decision tree based models are widely used in practice due to their robustness, interpretability, and ability to handle missing data.  
  
  To address this we present the design of \Treebeard{}, a schedule guided compiler infrastructure 
  for decision tree based models that searches over a large design space 
  to find high performance schedules on a diverse set of CPU and GPU 
  targets. 
  %We re-architect the open-source \TreebeardOLD{} infrastructure and significantly extend it to enable high-performance code generation across target processors. 
  \Treebeard{} has two core components. A scheduling language 
  that encapsulates the large optimization space for decision tree inference, 
  and techniques to efficiently explore this space.
  \TODO{include some keywords from the language and space complexity}.
  %We also design a set of heuristics that can find near-optimal solutions in quick time.
  Second, an optimizing multi-level compiler that can generate code based on the schedule. 
  For the latter, we re-architect the open-source \TreebeardOLD{} CPU compiler to support schedule guided compilation and
add support for GPU code generation. We further enhance the compiler with fundamental new optimization interfaces for caching, parallel reduction, and a plug-in mechanism to explore different in-memory representations of trees.
  
  %The compiler builds on an existing compiler for CPUs, \TreebeardOLD{}, also add support for caching, parallel reduction and a plug-in mechanism to explore different in-memory representations of trees.

  We evaluate \Treebeard{} on \TODO{kr:Can we quote a number} a large number of diverse models and demonstrate that the best schedule varies drastically with model, batch size, and target hardware. 
  Despite this, the scheduling heuristic is able to quickly find near optimal schedules while searching over a small fraction of the total search space.
  In terms of performance, \Treebeard{} generated code  
   is an order of magnitude faster than XGBoost and
  about 2-3$\times$ faster on average than RAPIDs FIL and Tahoe. \TODO{Make 
  this more precise. But how?}. While these systems only generate code for Nvidia, \Treebeard{} achieves competent performance on AMD GPUs as well. 
  On CPUs, we compare against \TreebeardOLD{} and show how the  
  optimization opportunities exposed by \Treebeard{}'s scheduling 
  language allow it to generate higher performance code by specializing  
  code for different batch sizes.
  \TODO{(numbers for CPU performance?)}
\end{abstract}