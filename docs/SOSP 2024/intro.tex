\section{Introduction}
\label{sec:intro}
We are in the midst of a hardware revolution, a new golden age for computer architecture~\cite{GoldenAge}. The last 
decade has seen a shift in architectural paradigms, with the rise of GPUs and accelerators. This shift has been driven
by the necessity to innovate in the post Moore's law and Dennard's scaling era. This transformation has also played 
a significant role in the success of modern deep learning models, as they enable scaling model training and inference 
to a massive number of threads. Such scalability would be essential for all performance critical applications, including
other machine learning models that need to scale with increasing data sizes and model complexities. 
%need to accelerate machine learning workloads. The success of modern AI models is due to their ability to scale both training and inference to a massive number of threads. This has been made possible by the rise of GPUs and accelerators.

%Despite these recent advancements, one dominant family of models, namely decision forest models have not seen the same level of acceleration.
Decision forest models remain the mainstay for machine learning over tabular data~\cite{DLNotAllYouNeed,TreebasedOutperformDL}. 
Their robustness, interpretability, and ability to handle missing data make them a popular choice for a wide range of applications. 
\TODO{a few more lines, classification, regression, etc.}
%We observe that while scalable performance on neural network models has received significant attention, decision tree inference has not seen the same level of acceleration.
A recent survey~\cite{kaggle}\dots. 
The survey also finds that, the cost of inference is the most critical factor in the overall cost of deploying a machine learning model.
This is because, in production settings, each model is trained once and often used for inference a few thousand times. 
%

This paper is motivated by the need to accelerate decision tree inference to achieve portable performance on a variety of client hardware.
In particular, we focus on portable performance across a range of commodity CPUs and low-to-mid-range GPUs, a class of hardware that has seen widespread adoption across client/edge devices used for inference.

Decision forest models are composed of a large collection of decision trees (100-1000), and inference involves traversing down each tree in the forest and aggregating the predictions. Inference is typically done in a batched setting, where multiple inputs are processed simultaneously.

%Despite the simplicity of the model, decision tree inference is computationally expensive, and is surprisingly hard to scale to a large number of threads
Despite the simplicity of the model and the availability of multiple sources of coarse grain parallelism (parallelism across inputs in a batch and parallelism across trees), existing systems do not consistently scale well across different models. 
This is because existing systems exploit a limited set of optimizations and often specialize the implementation to a specific hardware platform, which limits their portability. 

%\TODO{cover Rapids, Tahoe, XGBoost and Treebeard}.  
Evaluation on a diverse set of models highlights that the best implementation often requires a careful combination of many optimization strategies. For example, managing the working set of nodes across a set of trees is critical to get scalable performance. This requires a combination of techniques like data layout optimizations, loop transformations, and memory access optimizations. Existing systems each use specific data layouts for the underlying tree (XGBoost uses a sparse representation, RAPIDs FIL uses what is called the reorg representation 
and Tahoe uses a variation of the reorg representation) and propose simple loop transformations (XGBoost uses \dots()) that together determine the accesses patterns and the working set. They do not perform consistently well across different models and hardware platforms.
Managing the working set is just one of several optimizations that are critical for high-performance decision tree inference.
 \TODO{Improve the next line}
 Others include tree ordering,  incremental reduction of predicted values, interleaving across multiple trees, and so on.

This paper presents \Treebeard{}, a novel schedule guided compilation infrastructure for decision tree inference on multiple target hardware. \Treebeard{} is able to generate high-performance code for decision tree inference by exploring a large optimization space. The code generation is guided by a \emph{schedule} object that allows us to effectively represent this solution space abstractly. The schedule object is written in \Treebeard{} custom scheduling language that is expressive enough to represent a wide range of implementation strategies. We demonstrate that the language is sufficient to express the various optimizations proposed by prior work, and further generalizes them and incorporates several new optimizations. We also design and implement a heuristic that is able to quickly find high-performance schedules for the model being compiled.
\TODO{Performance evaluation summary}
\TODO{Oganization}
\subsection{Contributions}
\begin{itemize}
  \item We present the design for a multi-target decision tree compiler infrastructure and implement several optimizations within this framework.
   We are also the first to implement an optimizing compiler for decision tree inference on GPUs.
  \item We identify that an extensive optimization space exists for the problem of decision tree inference. We design a scheduling language that 
  allows us to effectively represent this solution space abstractly. This scheduling language is expressive enough to represent a wide range of  
  implementation strategies proposed by prior work.
  \item To the best of our knowledge, we perform the first extensive characterization of the optimization space for decision tree inference 
  on GPUs. Using some of the characteristics we identify, we design and implement a heuristic that is able to quickly find high-performance 
  schedules for the model being compiled. 
  \item We design and implement a general framework for expressing and optimizing reductions within MLIR. To the best of our knowledge, this is the first
  such framework.
  \item We evaluate our implementation by comparing it against RAPIDs and Tahoe, the state-of-the-art decision tree inference frameworks for GPU and 
  report significant speedups. We also show that our compiler can effectively target different GPUs, including both NVIDIA and AMD GPUs.
\end{itemize}