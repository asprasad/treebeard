Q1: How does SilvanForge’s decision-forest-based ‘schedule space’ and scheduling language differ from prior languages for other domains, e.g., TVM’s? For instance, what operators are new to SilvanForge?

One of our paper's core contributions is identifying that a schedule space exists for decision tree inference, showing that there is a huge variation in performance depending on which schedule is picked and  then designing a scheduling language that cleanly encapsulates this space on both CPUs and GPUs. To the best of our knowledge, ours is the first work to study the design of a scheduling language for the domain of decision tree inference. While several prior works proposed various techniques to optimize decision tree inference, all of them (RAPIDs, Tahoe, XGBoost etc) use a fixed set of techniques. In contrast, SilvanForge’s scheduling language is general enough to represent the specific optimizations that are used in several previous systems and therefore unifies several existing techniques. 

SilvanForge's scheduling language is novel because of the non-obvious realization that a suitably enhaced loop centric scheduling language elegantly encapsulates the optimization space for an irregular application like decision tree inference. SilvanForge’s scheduling language naturally adds scheduling primitives that are specific to decision tree inference (to represent walk interleaving, walk unrolling, reductions etc.) which cannot be supported with existing scheduling languages. This is reminiscent of how TVM provided tensor specific primitives, namely tensorization and cooperative data loading.  We believe that the design of SilvanForge's scheduling language is conceptually satisfying because it shows that extending a core set of loop transformation primitives with a few domain specific primitives can capture the optimization space of a complex application like decision tree inference (similar to how TVM extensions to the scheduling language were sufficient to compile tensor based applications).

Q2: Which SilvanForge optimizations are least likely to be supported by PyTorch (in the context of a decision forest compiled by modern Hummingbird), and why?
* Mention how tensor representations are not a suitable abstraction to express all the transformations that SF can do
* HB executes one walk per thread and fully walks all trees for a row in a single thread (as far as I can see from the paper). Therefore, SilvanForge will be able to find better schedules. 
* Expressing tree walks as a set of tensor operations makes it impossible for the compiler to perform optimizations like walk unrolling and walk interleaving. Also, this would necessitate several kernel calls (for each step of the tree walk and then subseqently for reductions). This representation also means that HB cannot perform optimizations like reductions in shared memory which SilvanForge can. 
* HB is hard-coded to use one kind of representation which is not ideal for all models and target machines. 
* TODO should we also say how TB cannot match up on the CPU? And focus on how SF improves even on TB with the schedule? 

RB Q: You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?
A: The scheduling language, HIR and MIR parts of the compiler are fully shared. Most of the backend is shared as well. Maybe a good idea to talk about how we share low level code gen between CPU and GPU? 
We found that a search heuristic is not needed on the GPU. Exploring only a few configurations is sufficient to find the best schedule. 

RC Q: Although I wonder if at least some of the explanation is that so many people have moved on from decision tree models to the hot new generative thing.

RC Q: It's really surprising to me that MLIR doesn't support reductions well -- they seem pretty fundamental to many kernels that other compilers targeting MLIR would see. But this section doesn't really explain if there's anything specific to the decision tree world here, or if this is just a general-purpose "add reductions to MLIR" thing. Either is probably interesting, but I didn't get enough context from the paper to know the answer.

RE Q: How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
A: TODO Mention what we've said about perf improvements due to shared reduction and row caching? 

RE Q: There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model.
While we did not evaluate such metrics, it should not be difficult to optimize for any other such metrics. We believe that the scheduling language is expressive enough to optimize for other metrics.

RD Q: ...speedup diminishes on large batch sizes.

RC Q:  It would be helpful to see results at batch size 1, for example, even if they're not great.
TODO: Should we say that we would use CPUs for such small batch sizes?
-------------------------------------------------------------------------

Reviewer A

* [Done] There is very little conceptual novelty; similar approaches are widely used for neural network optimization and graphics code (Halide).
* Very limited evaluation (only 8 real-world benchmarks), comparison to important related work appears to be missing.
* Past work has focused on throughput as opposed to latency, and I think this paper seems to conflate them somewhat. I think the evaluation would be stronger if it separately looked at throughput and latency; it may be possible that optimizations in the scheduling space optimize one at the cost of the other.
* [Done] The main issue, in my view, is that the paper is missing a comparison against Hummingbird

Reviewer B
* [Done] Table 5 (scheduler search space) and Algorithm 1 (search heuristic) seem rather GPU specific. You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?

Reviewer C
* [Done] Not clear about novelty over other scheduling languages
* Autotuning is ad hoc

Reviewer D
* Evaluated only on benchmarks. Unclear how well it will work in a practical setup. Specifically, its speedup diminishes on large batch sizes.
* Can you show the absolute time? All the numbers are shown as relative speedups. But if the absolute time is already fast enough, how much do practitioners care about the improved inference performance?

Reviewer E
* [Done] Unclear how the ideas here are different from ideas presented in other programmer-directed scheduling systems like Halide.
* [Done] There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model. In particular, the choice of representation (in Section 6) might affect this footprint. I would have liked to see an evaluation of this.
* [Done] How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
* [Done] Could you comment on the main decision-tree-specific insights in the scheduling language here in comparison to prior scheduling languages for other domains?
