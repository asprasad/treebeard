Q1: How does SilvanForge’s decision-forest-based ‘schedule space’ and scheduling language differ from prior languages for other domains, e.g., TVM’s? For instance, what operators are new to SilvanForge?

Unlike other domains where scheduling languages are used, SilvanForge targets an irregular application, one that involves traversing multiple decision trees. One of the paper's key contributions lies in recognizing the existence of a scheduling space for a domain as diverse as ours and demonstrating the significant performance variations across different schedules. The proposed scheduling language suitably enhances a loop centric scheduling language to target SilvanForge specific operators and perform tree traversal specific optimizations like walk interleaving, walk unrolling, reductions etc. These cannot be supported with existing scheduling languages. {Can we show an example.}
This has parallels to how TVM extends Halide with DNN compilation specific primitives like tensorization and cooperative data loading. Despite the fact that TVM and Halide operate on similar domains (deep learning vs. image processing), such extensions were necessary for TVM to perform optimizations that were not expressible in Halide.

We believe that the design of SilvanForge's scheduling language is conceptually satisfying because it shows that adding the required domain-specific primitives to a core set of loop transformation primitives can capture the optimization space of a complex application like decision tree inference. Prior work has proposed a small fixed set of optimization techniques and as discussed in the paper, SilvanForge’s scheduling language is powerful enough to express all these optimizations and more. 

Q2: Which SilvanForge optimizations are least likely to be supported by PyTorch (in the context of a decision forest compiled by modern Hummingbird), and why?
A: 
Hummingbird utilizes PyTorch by translating decision tree inference computation to a PyTorch graph. While this enables GPU compilation and re-targetability, as we discuss below the tensor abstraction is not sufficient to express rewrites that SilvanForge performs. 
The Hummingbird translation exposes all forms of parallelism to the tensor compiler. The generated code performs all tree walks in parallel, stores the results of individual tree walks in a tensor and finally performs a reduction. In the process Hummingbird performs a limited form of tree-walk unrolling (it pads all trees to the same depth and unrolls walks). This as discussed in our paper is not always useful (say when?). It is very difficult to express other optimizations including tree walk interleaving, caching trees/rows and reductions in shared memory, using tensor operations. 

Representing tree traversals through tensor operations obscures the underlying computation's semantics, rendering it unfeasible for the tensor compiler to perform these optimizations either. More specifically, the generated tensor code relies on gathering individual features when required. With this representation the analysis of memory access and reuse is complicated due to the indirect accesses of trees and rows. This makes it impossible for the tensor compiler to read input rows into shared memory as SilvanForge does. Second, to the best of our knowledge, tensor compilers cannot perform interleaving as SilvanForge does (this is not part of their schedule space). Third, we expect that the tensor representation will necessitate multiple kernel calls (at least one for the tree walk and then subsequently for the reduction) which will be inefficient compared to the single kernel call that SilvanForge can generate not only because of the kernel call overhead, but also because intermediate results will need to be written to memory (and not to shared memory as in the case of SilvanForge). 
Finally, it would be nearly impossible to restructure the computation to correctly determine the optimal number of threads to use for tree and row parallelism since no template schedules can be specified for the kernels finally generated by the tensor compiler. SilvanForge addresses these problems with domain specific abstractions and knowledge. 

The above discussion brings up a more practical system level problem ... 
* Systems challenges 
    -- no access to lower level details like shared memory, thread synchronization, etc at the tensor abstraction
    -- kernels are generated dynamically from the tensor graph and there is no scope for controlling how these kernels are optimized at the level of the tensor graph. TVM scheduling language is lower level. 

Another aspect where SilvanForge and Hummingbird differ is support for different representations of the model. SilvanForge can generate code for different representations of the model (array, sparse, reorg) while Hummingbird only supports a single representation. As noted in the paper, different representations can have different performance characteristics and SilvanForge can tune the code to use the representation that is best suited for the model being compiled. 

These views are partially validated by prior experiments on CPUs where a direct comparison between Treebeard (that performs optimizations like unrolling and interleaving) and Hummingbird reveals a massive performance gap. It is also not possible to express the various GPU specific configurations that SilvanForge can express, like parallelism on both tree and row, caching and reduction, with just tensor abstractions. 

* We find that in some cases, padding trees is very detrimental to performance (almost 2x in some cases). HB currently always pads trees to the same depth. SilvanForge can generate code that does not pad trees. Additionally, even when trees are padded, HB forces all trees in the model to have the same depth while this is not the case with SilvanForge.

RB Q: You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?
A: The scheduling language, high-level IR and mid-level IR are fully shared.
As described in the paper, except for a few target specific primitives, the scheduling language is common for both target processors. The same scheduling language is used to specify the structure of inference code on the CPU. When a schedule doesn't mark any loops as targetting a GPU dimension, CPU code is generated. The high-level and mid-level IR and optimizations on them are fully shared between the CPU and GPU compilation pipelines. The inference computation is represented as loops in both these IRs and therefore the, the representations and transformations on them are common. The compilation pipelines diverge starting starting at the lowering of MIR to low-level IR (as described in the paper). Even so, we are able to share significant amount of code between the CPU and GPU lowering due to how in-memory representations are abstracted in SilvanForge (Section 6). We found that a search heuristic is not needed on the CPU. Exploring only a few configurations is sufficient to find the best schedule. For the CPUs we tested on, we found that applying all optimizations is always beneficial and only a few configurations of the loop-nest needs to be explored.

RC Q: Although I wonder if at least some of the explanation is that so many people have moved on from decision tree models to the hot new generative thing.
A: As we establish in Section 1, decision trees are still widely used in practice and are likely to remain so for the foreseeable future. Also, the fact that their popularity has not declined over the last few years is evident from the last few Kaggle surveys. We believe that the retargetable performance that SilvanForge provides will be valuable to the large number of practitioners who use decision trees.

RC Q: It's really surprising to me that MLIR doesn't support reductions well -- they seem pretty fundamental to many kernels that other compilers targeting MLIR would see. But this section doesn't really explain if there's anything specific to the decision tree world here, or if this is just a general-purpose "add reductions to MLIR" thing. Either is probably interesting, but I didn't get enough context from the paper to know the answer.
A: As mentioned in the paper, the parallel reduction support in MLIR currently is either for value types (scf.reduce) or for tensors (linalg). Both of these do not fit the requirements of generating code for decision tree inference. With scf.reduce, every accumulation creates a logical copy and eliminating these copies correctly is a non-trivial problem. Additionally, MLIR does not have a lowering path from scf.reduce to a GPU implementation. With linalg, the entire tensor of values that are to be reduced need to be written to memory first thus making it impossible to fuse the reduction into the inner-most loops.

The reduction support we implement in SilvanForge is fairly general. Only the process of identifying reduction loops (Section 5) is specific to decision tree inference (tree loops are identified as reduction loops). All lowering of the ops in the reduction dialect are general and can be used for any other application that requires reductions. \TODO{Should we say that we can replace the reduction loop identification with a more general mechanism?}

RE Q: There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model.
A: While we did not evaluate such metrics, it should not be difficult to optimize for them using SilvanForge. We believe that the generality of the scheduling language encapsulates a vast enough space and finding variants that are optimized for other metrics should be possible.

-------------------------------------------------------------------------

RE Q: How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
A: TODO Mention what we've said about perf improvements due to shared reduction and row caching?
RD Q: ...speedup diminishes on large batch sizes.

RC Q:  It would be helpful to see results at batch size 1, for example, even if they're not great.
TODO: Should we say that we would use CPUs for such small batch sizes?

Reviewer A

* [Done] There is very little conceptual novelty; similar approaches are widely used for neural network optimization and graphics code (Halide).
* Very limited evaluation (only 8 real-world benchmarks), comparison to important related work appears to be missing.
* Past work has focused on throughput as opposed to latency, and I think this paper seems to conflate them somewhat. I think the evaluation would be stronger if it separately looked at throughput and latency; it may be possible that optimizations in the scheduling space optimize one at the cost of the other.
* [Done] The main issue, in my view, is that the paper is missing a comparison against Hummingbird

Reviewer B
* [Done] Table 5 (scheduler search space) and Algorithm 1 (search heuristic) seem rather GPU specific. You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?

Reviewer C
* [Done] Not clear about novelty over other scheduling languages
* Autotuning is ad hoc

Reviewer D
* Evaluated only on benchmarks. Unclear how well it will work in a practical setup. Specifically, its speedup diminishes on large batch sizes.
* Can you show the absolute time? All the numbers are shown as relative speedups. But if the absolute time is already fast enough, how much do practitioners care about the improved inference performance?

Reviewer E
* [Done] Unclear how the ideas here are different from ideas presented in other programmer-directed scheduling systems like Halide.
* [Done] There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model. In particular, the choice of representation (in Section 6) might affect this footprint. I would have liked to see an evaluation of this.
* [Done] How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
* [Done] Could you comment on the main decision-tree-specific insights in the scheduling language here in comparison to prior scheduling languages for other domains?
