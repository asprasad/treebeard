We thank the reviewers for their insightful reviews.

_Q1: Novelty of scheduling language._

Unlike other domains where scheduling languages are used, SilvanForge targets an irregular application, one that involves traversing multiple decision trees. One of the paper's key contributions lies in recognizing the existence of a scheduling space for this domain and demonstrating the significant performance variations across different schedules. The proposed scheduling language suitably enhances a loop centric scheduling language to target SilvanForge specific operators and perform tree traversal specific optimizations like tree-walk interleaving, tree-walk unrolling, reduction optimizations etc. These cannot be supported with existing scheduling languages. This has parallels to how TVM extends Halide with domain-specific primitives for DNN compilation like tensorization and cooperative data loading. Despite the fact that TVM and Halide operate on similar domains, such extensions were necessary for TVM to perform optimizations that were not expressible in Halide.

We believe that the design of SilvanForge's scheduling language is conceptually satisfying because it shows that adding a few required domain-specific primitives to a core set of loop transformation primitives can capture the optimization space of a complex application like decision tree inference. Prior work has proposed a small fixed set of optimization techniques, and as discussed in the paper, SilvanForge’s scheduling language is powerful enough to express all these optimizations and more. 

_Q2: Comparison with Hummingbird, PyTorch etc._

Hummingbird translates decision tree inference into a series of tensor operations to use PyTorch. While this enables re-targetability, the tensor abstraction is insufficient to express all SilvanForge optimizations. In the latest Hummingbird version, the generated tensor code performs all tree walks in parallel, stores the individual tree predictions in a tensor and finally performs a reduction. Hummingbird performs a limited form of tree-walk unrolling (it pads all trees to the same depth and unrolls walks), which is not always useful (Section 2). However, it is very difficult to express other SilvanForge optimizations including tree walk interleaving, caching and reduction optimizations, using tensor operations. 

Further, using tensor operations to represent tree traversals obscures the computation's semantics, making it difficult for the tensor compiler to perform these optimizations. Consider the example of walk-interleaving. SilvanForge’s tree-walk interleaving relies on unrolling only the trees whose walks are being interleaved to the same depth. It also reorders trees so that trees with the same depth are processed together. While it maybe possible to extend tensor compilers to perform interleaving, it will be very hard to mimic what SilvanForge does. Current tensor compilers optimize dense matrix computation and do not have the optimizations necessary to interleave instructions across iterations. Even if this support is added, picking the right tree-walks (same tree different rows, different trees same row etc.) to interleave would be a non-trivial problem.  

The tensor compiler cannot perform the required caching and reduction optimizations either. Since the generated tensor code relies on gathering individual features when required, the indirect accesses to trees and rows complicates the analysis of memory access and reuse. This makes it impossible for the tensor compiler to read input rows into shared memory as SilvanForge does. Additionally, we expect that the tensor representation will necessitate multiple kernel calls (at least one for the tree walks and one for the reduction) which will be inefficient compared to the single kernel call that SilvanForge generates.

The above discussion brings up a more practical system-level problem. Expressing inference using PyTorch means that the decision tree compiler has no control over lower-level details like shared memory, thread synchronization, etc. Kernels are generated dynamically and there is no scope for controlling how these kernels are optimized. Systems like TorchDynamo and the recently introduced TorchInductor (as per the ASPLOS 2024 paper and other docs) do not provide this level of control to the PyTorch program. 

These views are partially validated by experiments in previous literature where, on CPUs, a direct comparison between Treebeard (that performs optimizations like unrolling and interleaving) and Hummingbird reveals an order-of-magnitude performance gap. We expect a future comparison of Hummingbird and SilvanForge on GPUs to show similar results.

_Rev#D: Practical applicability._

We are currently engaging with industry practitioners and library code owners who are keen to adopt SilvanForge, inspired by the promising results we’ve demonstrated. Their requirements include maximizing performance across a range of batch sizes, as their application use-cases vary. 

_Rev#A: Benchmarks._

We used the benchmark set from previous work and included a large number of synthetic models with variations in depth, number of trees, and number of features to provide additional coverage. Our results are consistent across all these models, indicating robustness. 

_Rev#A: Absolute times._

The reported numbers are the mean execution time per row averaged over the batch. Since several rows are processed in parallel on the GPU, it is challenging to directly correlate the reported times with single operations.

_Rev#B: Portability._

The scheduling language, HIR and MIR and optimizations on them are fully shared. The same scheduling language is used to specify the structure of inference code on CPUs and GPUs. When a schedule doesn't mark any loops as targetting a GPU dimension, CPU code is generated. The compilation pipelines diverge starting at the lowering of MIR to LIR. Even so, we are able to share a significant amount of code between the CPU and GPU lowering because of how in-memory representations are abstracted (Section 6).

_Rev#C: Support for reductions._

MLIR currently supports parallel reductions for value types (scf.reduce) and reducing dimensions of tensors (linalg.reduce). Neither of these meets the requirements for generating code for decision tree inference. With scf.reduce, every accumulation creates a logical copy and eliminating these copies correctly is a non-trivial problem. With linalg.reduce, the reduction cannot be fused with another loop as the entire tensor of values that are to be reduced need to be written to memory first. The reduction support we implement in SilvanForge is fairly general. Only the process of identifying reduction loops is currently specific to decision tree inference. All lowering of the ops in the reduction dialect are general.

-------------------------------------------------------------------------

Another aspect where SilvanForge and Hummingbird differ is support for different representations of the model. SilvanForge can generate code for different representations of the model (array, sparse, reorg) while Hummingbird only supports a single representation. As noted in the paper, different representations can have different performance characteristics and SilvanForge can tune the code to use the representation that is best suited for the model being compiled. 

Finally, it would be nearly impossible to restructure the computation to correctly determine the optimal number of threads to use for tree and row parallelism since no template schedules can be specified for the kernels finally generated by the tensor compiler. SilvanForge addresses these problems with domain specific abstractions and knowledge. 

Rev#C: Have people moved on from decision tree models?
As we discuss in the paper, decision trees are widely used in practice. Also, the fact that their popularity has not declined over the last few years is evident from the last few Kaggle surveys. We believe that the retargetable performance that SilvanForge provides will be valuable to the large number of practitioners who use decision trees.

Rev#E: Other metrics besides kernel time speedup. 
While we did not evaluate such metrics, it should not be difficult to optimize for them using SilvanForge. We believe that the generality of the scheduling language encapsulates a vast enough space and finding variants that are optimized for other metrics should be possible.

RE Q: How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
A: TODO Mention what we've said about perf improvements due to shared reduction and row caching?
RD Q: ...speedup diminishes on large batch sizes.

RC Q:  It would be helpful to see results at batch size 1, for example, even if they're not great.
TODO: Should we say that we would use CPUs for such small batch sizes?

Reviewer A

* [Done] There is very little conceptual novelty; similar approaches are widely used for neural network optimization and graphics code (Halide).
* Very limited evaluation (only 8 real-world benchmarks), comparison to important related work appears to be missing.
* Past work has focused on throughput as opposed to latency, and I think this paper seems to conflate them somewhat. I think the evaluation would be stronger if it separately looked at throughput and latency; it may be possible that optimizations in the scheduling space optimize one at the cost of the other.
* [Done] The main issue, in my view, is that the paper is missing a comparison against Hummingbird

Reviewer B
* [Done] Table 5 (scheduler search space) and Algorithm 1 (search heuristic) seem rather GPU specific. You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?

Reviewer C
* [Done] Not clear about novelty over other scheduling languages
* Autotuning is ad hoc

Reviewer D
* Evaluated only on benchmarks. Unclear how well it will work in a practical setup. Specifically, its speedup diminishes on large batch sizes.
* Can you show the absolute time? All the numbers are shown as relative speedups. But if the absolute time is already fast enough, how much do practitioners care about the improved inference performance?

Reviewer E
* [Done] Unclear how the ideas here are different from ideas presented in other programmer-directed scheduling systems like Halide.
* [Done] There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model. In particular, the choice of representation (in Section 6) might affect this footprint. I would have liked to see an evaluation of this.
* [Done] How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
* [Done] Could you comment on the main decision-tree-specific insights in the scheduling language here in comparison to prior scheduling languages for other domains?
