Q1: Novelty of scheduling language.

Unlike other domains where scheduling languages are used, SilvanForge targets an irregular application, one that involves traversing multiple decision trees. One of the paper's key contributions lies in recognizing the existence of a scheduling space for a domain as diverse as ours and demonstrating the significant performance variations across different schedules. The proposed scheduling language suitably enhances a loop centric scheduling language to target SilvanForge specific operators and perform tree traversal specific optimizations like tree-walk interleaving, tree-walk unrolling, reductions etc. These cannot be supported with existing scheduling languages. This has parallels to how TVM extends Halide with domain-specific primitives for DNN compilation like tensorization and cooperative data loading. Despite the fact that TVM and Halide operate on similar domains (deep learning vs. image processing), such extensions were necessary for TVM to perform optimizations that were not expressible in Halide.

We believe that the design of SilvanForge's scheduling language is conceptually satisfying because it shows that adding the required domain-specific primitives to a core set of loop transformation primitives can capture the optimization space of a complex application like decision tree inference. Prior work has proposed a small fixed set of optimization techniques, and as discussed in the paper, SilvanForge’s scheduling language is powerful enough to express all these optimizations and more. 

Q2: Comparison with Hummingbird

Hummingbird translates the decision tree inference into a series of tensor operations to use PyTorch. While this enables re-targetability, the tensor abstraction is insufficient to express all SilvanForge optimizations. Hummingbird exposes all forms of parallelism to the tensor compiler. The generated tensor code performs all tree walks in parallel, stores the individual tree predictions in a tensor and finally performs a reduction. In the process, Hummingbird performs a limited form of tree-walk unrolling (it pads all trees to the same depth and unrolls walks). This, as discussed in our paper, is not always useful (Section 2). It is very difficult to express other optimizations including tree walk interleaving, caching trees/rows and reductions in shared memory, using tensor operations. 

Representing tree traversals through tensor operations obscures the underlying computation's semantics, rendering it infeasible for the tensor compiler to perform these optimizations. Consider the example of walk-interleaving. SilvanForge’s tree-walk interleaving relies on unrolling only the trees whose walks are being interleaved to the same depth. It also reorders trees so that trees with the same depth are processed together. While it maybe possible to extend tensor compilers to perform interleaving, it will be very hard to mimic what SilvanForge does. Current tensor compilers optimize dense matrix computation and do not have the optimizations necessary to interleave instructions across iterations. Even if they did, picking the right tree-walks (same tree different rows, different trees same row etc.) to interleave would be a non-trivial problem.  

The tensor compiler cannot perform the required caching and reduction optimizations either. Since the generated tensor code relies on gathering individual features when required, the indirect accesses of trees and rows complicates the analysis of memory access and reuse. This makes it impossible for the tensor compiler to read input rows into shared memory as SilvanForge does. Additionally, we expect that the tensor representation will necessitate multiple kernel calls (at least one for the tree walk and then subsequently for the reduction) which will be inefficient compared to the single kernel call that SilvanForge generates.

The above discussion brings up a more practical system level problem. Expressing the tree inference using PyTorch means that the decision tree compiler has no control over lower level details like shared memory, thread synchronization, etc. Kernels are generated dynamically and there is no scope for controlling how these kernels are optimized. Systems like TorchDynamo and TorchInductor do not provide this level of control to the PyTorch program. 

These views are partially validated by experiments in previous literature where, on CPUs, a direct comparison between Treebeard (that performs optimizations like unrolling and interleaving) and Hummingbird reveals an order-of-magnitude performance gap. 

Rev#B: Are your compiler changes 'portable' across platforms?
The scheduling language, high-level IR and mid-level IR are fully shared. The same scheduling language is used to specify the structure of inference code on CPUs and GPUs. When a schedule doesn't mark any loops as targetting a GPU dimension, CPU code is generated. The high-level and mid-level IR and optimizations on them are fully shared between the CPU and GPU compilation pipelines. The inference computation is represented as loops in both these IRs and therefore the, the representations and transformations on them are common. The compilation pipelines diverge starting starting at the lowering of MIR to low-level IR. Even so, we are able to share significant amount of code between the CPU and GPU lowering due to how in-memory representations are abstracted in SilvanForge (Section 6). We found that a search heuristic is not needed on the CPU. Exploring only a few configurations is sufficient to find the best schedule. For the CPUs we tested on, we found that applying all optimizations is always beneficial and only a few configurations of the loop-nest needs to be explored.

Rev#C: Have people moved on from decision tree models?
As we establish in Section 1, decision trees are still widely used in practice and are likely to remain so for the foreseeable future. Also, the fact that their popularity has not declined over the last few years is evident from the last few Kaggle surveys. We believe that the retargetable performance that SilvanForge provides will be valuable to the large number of practitioners who use decision trees.

Rev#C: Support for reductions.
As mentioned in the paper, the parallel reduction support in MLIR currently is either for value types (scf.reduce) or for tensors (linalg). Both of these do not fit the requirements of generating code for decision tree inference. With scf.reduce, every accumulation creates a logical copy and eliminating these copies correctly is a non-trivial problem. Additionally, MLIR does not have a lowering path from scf.reduce to a GPU implementation. With linalg, the entire tensor of values that are to be reduced need to be written to memory first thus making it impossible to fuse the reduction into the inner-most loops.

The reduction support we implement in SilvanForge is fairly general. Only the process of identifying reduction loops (Section 5) is specific to decision tree inference (tree loops are identified as reduction loops). All lowering of the ops in the reduction dialect are general and can be used for any other application that requires reductions. \TODO{Should we say that we can replace the reduction loop identification with a more general mechanism?}

Rev#E: Other metrics besides kernel time speedup. 
While we did not evaluate such metrics, it should not be difficult to optimize for them using SilvanForge. We believe that the generality of the scheduling language encapsulates a vast enough space and finding variants that are optimized for other metrics should be possible.

-------------------------------------------------------------------------

Another aspect where SilvanForge and Hummingbird differ is support for different representations of the model. SilvanForge can generate code for different representations of the model (array, sparse, reorg) while Hummingbird only supports a single representation. As noted in the paper, different representations can have different performance characteristics and SilvanForge can tune the code to use the representation that is best suited for the model being compiled. 

Finally, it would be nearly impossible to restructure the computation to correctly determine the optimal number of threads to use for tree and row parallelism since no template schedules can be specified for the kernels finally generated by the tensor compiler. SilvanForge addresses these problems with domain specific abstractions and knowledge. 

RE Q: How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
A: TODO Mention what we've said about perf improvements due to shared reduction and row caching?
RD Q: ...speedup diminishes on large batch sizes.

RC Q:  It would be helpful to see results at batch size 1, for example, even if they're not great.
TODO: Should we say that we would use CPUs for such small batch sizes?

Reviewer A

* [Done] There is very little conceptual novelty; similar approaches are widely used for neural network optimization and graphics code (Halide).
* Very limited evaluation (only 8 real-world benchmarks), comparison to important related work appears to be missing.
* Past work has focused on throughput as opposed to latency, and I think this paper seems to conflate them somewhat. I think the evaluation would be stronger if it separately looked at throughput and latency; it may be possible that optimizations in the scheduling space optimize one at the cost of the other.
* [Done] The main issue, in my view, is that the paper is missing a comparison against Hummingbird

Reviewer B
* [Done] Table 5 (scheduler search space) and Algorithm 1 (search heuristic) seem rather GPU specific. You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?

Reviewer C
* [Done] Not clear about novelty over other scheduling languages
* Autotuning is ad hoc

Reviewer D
* Evaluated only on benchmarks. Unclear how well it will work in a practical setup. Specifically, its speedup diminishes on large batch sizes.
* Can you show the absolute time? All the numbers are shown as relative speedups. But if the absolute time is already fast enough, how much do practitioners care about the improved inference performance?

Reviewer E
* [Done] Unclear how the ideas here are different from ideas presented in other programmer-directed scheduling systems like Halide.
* [Done] There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model. In particular, the choice of representation (in Section 6) might affect this footprint. I would have liked to see an evaluation of this.
* [Done] How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
* [Done] Could you comment on the main decision-tree-specific insights in the scheduling language here in comparison to prior scheduling languages for other domains?
