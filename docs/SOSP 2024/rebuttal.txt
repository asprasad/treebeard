Q1: How does SilvanForge’s decision-forest-based ‘schedule space’ and scheduling language differ from prior languages for other domains, e.g., TVM’s? For instance, what operators are new to SilvanForge?

Unlike other domains where scheduling languages are used, SilvanForge targets an irregular application, one that involves traversing multiple decision trees. One of the paper's key contributions lies in recognizing the existence of a scheduling space for a domain as diverse as ours and demonstrating the significant performance variations across different schedules. The proposed scheduling language suitably enhances a loop centric scheduling language to target SilvanForge specific operators and perform tree traversal specific optimizations like walk interleaving, walk unrolling, reductions etc. These cannot be supported with existing scheduling languages. {Can we show an example.}
This has parallels to how TVM extends Halide to tensor specific primitives like tensorization and cooperative data loading. Despite the fact that TVM and Halide operate on similar domains (deep learning vs. image processing), such extensions were necessary for TVM to perform optimizations that were not expressible in Halide.

We believe that the design of SilvanForge's scheduling language is conceptually satisfying because it shows that by extending a core set of loop transformation primitives with a few domain specific primitives can capture the optimization space of a complex application like decision tree inference. Prior work has proposed a small fixed set of optimization techniques and as discussed in the paper, the language is powerful enough to express all these optimizations and more. 


Q2: Which SilvanForge optimizations are least likely to be supported by PyTorch (in the context of a decision forest compiled by modern Hummingbird), and why?
Hummingbird represents the decision tree inference computation as a PyTorch graph using tensor operations in order to take advantage of systems like PyTorch and TVM. While this enables GPU compilation and re-targetability, the tensor abstraction is not sufficient to express rewrites that SilvanForge performs. 
It is very difficult (or even impossible) to express many of our optimizations including tree walk interleaving , tree walk unrolling (especially when trees have different depths), caching trees/rows and reductions in shared memory, using tensor operations.
 \TODO{THis sounds too dismissive. Can we add a line saying what can be done with tensors. What would nail this is to show the tensor computation from HB and argue why its not possible to generate code corresponding to interleaved walk (or another optimization for which we show code) }
This is partially validated by prior experiments on CPUs where a direct comparison between Treebeard (that performs optimizations like unrolling and interleaving) and Hummingbird reveals a massive performance gap. 
It is also not possible to express the various GPU specific configurations that SilvanForge can express, like parallelism on both tree and row, caching and reduction, with just tensor abstractions. 

Additionally, representing tree traversals through tensor operations obscures the underlying computation's semantics, rendering it unfeasible for the tensor compiler to perform these optimizations. We expect that the tensor representation will necessitate multiple kernel calls (at least one for the tree walk and then subsequently for the reduction) which will be inefficient compared to the single kernel call that SilvanForge can generate not only because of the kernel call overhead, but also because intermediate results will need to be written to memory. 

The translation described in the Hummingbird paper uses one tensor element to represent the current node of each walk. This corresponds to the SilvanForge schedule where one thread walks one tree, and is only one of the many possible schedules that SilvanForge can generate (TODO check this). Another aspect where SilvanForge and Hummingbird differ is support for different representations of the model. SilvanForge can generate code for different representations of the model (array, sparse, reorg) while Hummingbird only supports a single representation. As noted in the paper, different representations can have different performance characteristics and SilvanForge can tune the code to use the representation that is best suited for the model being compiled. 


RB Q: You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?
A: The scheduling language, HIR and MIR parts of the compiler are fully shared. Most of the backend is shared as well. Maybe a good idea to talk about how we share low level code gen between CPU and GPU? 
We found that a search heuristic is not needed on the GPU. Exploring only a few configurations is sufficient to find the best schedule. 

RC Q: Although I wonder if at least some of the explanation is that so many people have moved on from decision tree models to the hot new generative thing.

RC Q: It's really surprising to me that MLIR doesn't support reductions well -- they seem pretty fundamental to many kernels that other compilers targeting MLIR would see. But this section doesn't really explain if there's anything specific to the decision tree world here, or if this is just a general-purpose "add reductions to MLIR" thing. Either is probably interesting, but I didn't get enough context from the paper to know the answer.

RE Q: How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
A: TODO Mention what we've said about perf improvements due to shared reduction and row caching? 

RE Q: There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model.
While we did not evaluate such metrics, it should not be difficult to optimize for any other such metrics. We believe that the scheduling language is expressive enough to optimize for other metrics.

RD Q: ...speedup diminishes on large batch sizes.

RC Q:  It would be helpful to see results at batch size 1, for example, even if they're not great.
TODO: Should we say that we would use CPUs for such small batch sizes?
-------------------------------------------------------------------------

Reviewer A

* [Done] There is very little conceptual novelty; similar approaches are widely used for neural network optimization and graphics code (Halide).
* Very limited evaluation (only 8 real-world benchmarks), comparison to important related work appears to be missing.
* Past work has focused on throughput as opposed to latency, and I think this paper seems to conflate them somewhat. I think the evaluation would be stronger if it separately looked at throughput and latency; it may be possible that optimizations in the scheduling space optimize one at the cost of the other.
* [Done] The main issue, in my view, is that the paper is missing a comparison against Hummingbird

Reviewer B
* [Done] Table 5 (scheduler search space) and Algorithm 1 (search heuristic) seem rather GPU specific. You claim that SilvanForge decouples schedule from the backend architecture  but it seems both of these would change substantially with CPU? What fraction of your compiler changes are 'portable' across platforms, and how well does the CPU search heuristic work?

Reviewer C
* [Done] Not clear about novelty over other scheduling languages
* Autotuning is ad hoc

Reviewer D
* Evaluated only on benchmarks. Unclear how well it will work in a practical setup. Specifically, its speedup diminishes on large batch sizes.
* Can you show the absolute time? All the numbers are shown as relative speedups. But if the absolute time is already fast enough, how much do practitioners care about the improved inference performance?

Reviewer E
* [Done] Unclear how the ideas here are different from ideas presented in other programmer-directed scheduling systems like Halide.
* [Done] There might be other metrics besides kernel time speedup. For instance, the memory footprint of the resulting model. In particular, the choice of representation (in Section 6) might affect this footprint. I would have liked to see an evaluation of this.
* [Done] How much do different optimizations contribute to overall performance of code generated by SILVANFORGE?
* [Done] Could you comment on the main decision-tree-specific insights in the scheduling language here in comparison to prior scheduling languages for other domains?
