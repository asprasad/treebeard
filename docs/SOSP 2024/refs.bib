@book{lamport94,
  author    = {Leslie Lamport},
  title     = {{\LaTeX: A Document Preparation System}},
  year      = {1994},
  publisher = {Addison-Wesley},
  edition   = {2nd},
  address   = {Reading, Massachusetts}
}

@inproceedings{LLVM,
  author    = {Lattner, C. and Adve, V.},
  booktitle = {International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
  title     = {LLVM: a compilation framework for lifelong program analysis  amp; transformation},
  year      = {2004},
  volume    = {},
  number    = {},
  pages     = {75-86},
  doi       = {10.1109/CGO.2004.1281665}
}

@inproceedings{MLIR,
  author    = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle = {2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  title     = {MLIR: Scaling Compiler Infrastructure for Domain Specific Computation},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {2-14},
  doi       = {10.1109/CGO51591.2021.9370308}
}

@inproceedings{XGBoost,
  author    = {Chen, Tianqi and Guestrin, Carlos},
  title     = {XGBoost: A Scalable Tree Boosting System},
  year      = {2016},
  isbn      = {9781450342322},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2939672.2939785},
  doi       = {10.1145/2939672.2939785},
  abstract  = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {785–794},
  numpages  = {10},
  keywords  = {large-scale machine learning},
  location  = {San Francisco, California, USA},
  series    = {KDD '16}
}

@inproceedings{LightGBM,
  author    = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  title     = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {3149–3157},
  numpages  = {9},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@inproceedings{TVM,
  author    = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
  title     = {{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  year      = {2018},
  isbn      = {978-1-939133-08-3},
  address   = {Carlsbad, CA},
  pages     = {578--594},
  url       = {https://www.usenix.org/conference/osdi18/presentation/chen},
  publisher = {USENIX Association},
  month     = oct
}

@inproceedings{Tiramisu,
  author    = {Baghdadi, Riyadh and Ray, Jessica and Romdhane, Malek Ben and Del Sozzo, Emanuele and Akkas, Abdurrahman and Zhang, Yunming and Suriana, Patricia and Kamil, Shoaib and Amarasinghe, Saman},
  title     = {Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code},
  year      = {2019},
  isbn      = {9781728114361},
  publisher = {IEEE Press},
  abstract  = {This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.},
  booktitle = {Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization},
  pages     = {193–205},
  numpages  = {13},
  keywords  = {Code Optimization, Distributed Systems, Code Generation, Polyhedral Model, Deep Learning, Tensors, GPU},
  location  = {Washington, DC, USA},
  series    = {CGO 2019}
}

@misc{XLA,
  key    = {XLA},
  author = {Google Inc},
  title  = {{XLA (Accelerated Linear Algebra) for TensorFlow}},
  note   = {https://www.tensorflow.org/performance/xla/},
  year   = 2017
}

@misc{KaggleSurvey,
  title        = {Kaggle State of Data Science and Machine Learning 2021},
  howpublished = {\url{https://www.kaggle.com/kaggle-survey-2021}},
  note         = {Accessed: 2022-04-16}
}

@misc{Treelite,
  title        = {Treelite : model compiler for decision tree ensembles},
  howpublished = {\url{https://treelite.readthedocs.io/en/latest/}},
  note         = {Accessed: 2022-04-16}
}

@misc{Sklearn,
  title        = {scikit-learn : Machine Learning in Python},
  howpublished = {\url{https://scikit-learn.org/stable/}},
  note         = {Accessed: 2022-04-16}
}

@misc{CUTLASS,
  title        = {NVIDIA CUTLASS},
  howpublished = {\url{https://github.com/NVIDIA/cutlass}},
  note         = {Accessed: 2022-04-16}
}

@inproceedings{Halide,
  author    = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}do and Amarasinghe, Saman},
  title     = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
  year      = {2013},
  isbn      = {9781450320146},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2491956.2462176},
  doi       = {10.1145/2491956.2462176},
  abstract  = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
  booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {519–530},
  numpages  = {12},
  keywords  = {optimization, domain specific language, vectorization, parallelism, locality, image processing, compiler, autotuning, redundant computation, gpu},
  location  = {Seattle, Washington, USA},
  series    = {PLDI '13}
}

@article{VPred,
  author  = {Asadi, Nima and Lin, Jimmy and de Vries, Arjen P.},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  title   = {Runtime Optimizations for Tree-Based Machine Learning Models},
  year    = {2014},
  volume  = {26},
  number  = {9},
  pages   = {2281-2292},
  doi     = {10.1109/TKDE.2013.73}
}

@inproceedings{QuickScorer,
  author    = {Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Tonellotto, Nicola and Venturini, Rossano},
  title     = {QuickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees},
  year      = {2015},
  isbn      = {9781450336215},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2766462.2767733},
  doi       = {10.1145/2766462.2767733},
  abstract  = {Learning-to-Rank models based on additive ensembles of regression trees have proven to be very effective for ranking query results returned by Web search engines, a scenario where quality and efficiency requirements are very demanding. Unfortunately, the computational cost of these ranking models is high. Thus, several works already proposed solutions aiming at improving the efficiency of the scoring process by dealing with features and peculiarities of modern CPUs and memory hierarchies. In this paper, we present QuickScorer, a new algorithm that adopts a novel bitvector representation of the tree-based ranking model, and performs an interleaved traversal of the ensemble by means of simple logical bitwise operations. The performance of the proposed algorithm are unprecedented, due to its cache-aware approach, both in terms of data layout and access patterns, and to a control flow that entails very low branch mis-prediction rates. The experiments on real Learning-to-Rank datasets show that QuickScorer is able to achieve speedups over the best state-of-the-art baseline ranging from 2x to 6.5x.},
  booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {73–82},
  numpages  = {10},
  keywords  = {cache-aware algorithms, learning to rank, efficiency},
  location  = {Santiago, Chile},
  series    = {SIGIR '15}
}

@inproceedings{QuickScorer1,
  author    = {Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Tonellotto, Nicola and Venturini, Rossano},
  title     = {Exploiting CPU SIMD Extensions to Speed-up Document Scoring with Tree Ensembles},
  year      = {2016},
  isbn      = {9781450340694},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2911451.2914758},
  doi       = {10.1145/2911451.2914758},
  abstract  = {Scoring documents with learning-to-rank (LtR) models based on large ensembles of regression trees is currently deemed one of the best solutions to effectively rank query results to be returned by large scale Information Retrieval systems. This paper investigates the opportunities given by SIMD capabilities of modern CPUs to the end of efficiently evaluating regression trees ensembles. We propose V-QuickScorer (vQS), which exploits SIMD extensions to vectorize the document scoring, i.e., to perform the ensemble traversal by evaluating multiple documents simultaneously. We provide a comprehensive evaluation of vQS against the state of the art on three publicly available datasets. Experiments show that vQS provides speed-ups up to a factor of 3.2x.},
  booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {833–836},
  numpages  = {4},
  keywords  = {ensemble methods, learning to rank, document scoring},
  location  = {Pisa, Italy},
  series    = {SIGIR '16}
}

@inproceedings{CacheConscious1,
  author    = {Tang, Xun and Jin, Xin and Yang, Tao},
  title     = {Cache-Conscious Runtime Optimization for Ranking Ensembles},
  year      = {2014},
  isbn      = {9781450322577},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2600428.2609525},
  doi       = {10.1145/2600428.2609525},
  abstract  = {Multi-tree ensemble models have been proven to be effective for document ranking. Using a large number of trees can improve accuracy, but it takes time to calculate ranking scores of matched documents. This paper investigates data traversal methods for fast score calculation with a large ensemble. We propose a 2D blocking scheme for better cache utilization with simpler code structure compared to previous work. The experiments with several benchmarks show significant acceleration in score calculation without loss of ranking accuracy.},
  booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research amp; Development in Information Retrieval},
  pages     = {1123–1126},
  numpages  = {4},
  keywords  = {query processing, ensemble methods, cache locality},
  location  = {Gold Coast, Queensland, Australia},
  series    = {SIGIR '14}
}

@inproceedings{CacheConscious2,
  author    = {Jin, Xin and Yang, Tao and Tang, Xun},
  title     = {A Comparison of Cache Blocking Methods for Fast Execution of Ensemble-Based Score Computation},
  year      = {2016},
  isbn      = {9781450340694},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2911451.2911520},
  doi       = {10.1145/2911451.2911520},
  abstract  = {Machine-learned classification and ranking techniques often use ensembles to aggregate partial scores of feature vectors for high accuracy and the runtime score computation can become expensive when employing a large number of ensembles. The previous work has shown the judicious use of memory hierarchy in a modern CPU architecture which can effectively shorten the time of score computation. However, different traversal methods and blocking parameter settings can exhibit different cache and cost behavior depending on data and architectural characteristics. It is very time-consuming to conduct exhaustive search for performance comparison and optimum selection. This paper provides an analytic comparison of cache blocking methods on their data access performance with an approximation and proposes a fast guided sampling scheme to select a traversal method and blocking parameters for effective use of memory hierarchy. The evaluation studies with three datasets show that within a reasonable amount of time, the proposed scheme can identify a highly competitive solution that significantly accelerates score calculation.},
  booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {629–638},
  numpages  = {10},
  keywords  = {ensemble methods, cache locality, query processing},
  location  = {Pisa, Italy},
  series    = {SIGIR '16}
}

@inproceedings{Hummingbird,
  author    = {Supun Nakandala and Karla Saur and Gyeong-In Yu and Konstantinos Karanasos and Carlo Curino and Markus Weimer and Matteo Interlandi},
  title     = {A Tensor Compiler for Unified Machine Learning Prediction Serving},
  booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  year      = {2020},
  isbn      = {978-1-939133-19-9},
  pages     = {899--917},
  url       = {https://www.usenix.org/conference/osdi20/presentation/nakandala},
  publisher = {USENIX Association},
  month     = nov
}

@inproceedings{ProbBasedLayout,
  author    = {Buschjager, Sebastian and Chen, Kuan-Hsun and Chen, Jian-Jia and Morik, Katharina},
  booktitle = {2018 IEEE International Conference on Data Mining (ICDM)},
  title     = {Realization of Random Forest for Real-Time Evaluation through Tree Framing},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {19-28},
  doi       = {10.1109/ICDM.2018.00017}
}

@inproceedings{Tahoe,
  author    = {Xie, Zhen and Dong, Wenqian and Liu, Jiawen and Liu, Hang and Li, Dong},
  title     = {Tahoe: Tree Structure-Aware High Performance Inference Engine for Decision Tree Ensemble on GPU},
  year      = {2021},
  isbn      = {9781450383349},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3447786.3456251},
  doi       = {10.1145/3447786.3456251},
  abstract  = {Decision trees are widely used and often assembled as a forest to boost prediction accuracy. However, using decision trees for inference on GPU is challenging, because of irregular memory access patterns and imbalance workloads across threads. This paper proposes Tahoe, a tree structure-aware high performance inference engine for decision tree ensemble. Tahoe rearranges tree nodes to enable efficient and coalesced memory accesses; Tahoe also rearranges trees, such that trees with similar structures are grouped together in memory and assigned to threads in a balanced way. Besides memory access efficiency, we introduce a set of inference strategies, each of which uses shared memory differently and has different implications on reduction overhead. We introduce performance models to guide the selection of the inference strategies for arbitrary forests and data set. Tahoe consistently outperforms the state-of-the-art industry-quality library FIL by 3.82x, 2.59x, and 2.75x on three generations of NVIDIA GPUs (Kepler, Pascal, and Volta), respectively.},
  booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
  pages     = {426–440},
  numpages  = {15},
  keywords  = {tree structure, performance model, decision tree ensemble, decision tree inference},
  location  = {Online Event, United Kingdom},
  series    = {EuroSys '21}
}

@inproceedings{MilindTreeVectorization,
  author    = {Jo, Youngjoon and Goldfarb, Michael and Kulkarni, Milind},
  title     = {Automatic Vectorization of Tree Traversals},
  year      = {2013},
  isbn      = {9781479910212},
  publisher = {IEEE Press},
  abstract  = {Repeated tree traversals are ubiquitous in many domains such as scientific simulation, data mining and graphics. Modern commodity processors support SIMD instructions, and using these instructions to process multiple traversals at once has the potential to provide substantial performance improvements. Unfortunately these algorithms often feature highly diverging traversals which inhibit efficient SIMD utilization, to the point that other, less profitable sources of vectorization must be exploited instead. Previous work has proposed traversal splicing, a locality transformation for tree traversals, which dynamically reorders traversals based on previous behavior, based on the insight that traversals which have behaved similarly so far are likely to behave similarly in the future. In this work, we cast this dynamic reordering as a scheduling for efficient SIMD execution, and show that it can dramatically improve the SIMD utilization of diverging traversals, close to ideal utilization. For five irregular tree traversal algorithms, our techniques are able to deliver speedups of 2.78 on average over baseline implementations. Furthermore our techniques can effectively SIMDize algorithms that prior, manual vectorization attempts could not.},
  booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
  pages     = {363–374},
  numpages  = {12},
  keywords  = {SIMD, tree traversals, automatic vectorization, irregular programs},
  location  = {Edinburgh, Scotland, UK},
  series    = {PACT '13}
}

@article{PortableVM,
  author     = {Ren, Bin and Mytkowicz, Todd and Agrawal, Gagan},
  title      = {A Portable Optimization Engine for Accelerating Irregular Data-Traversal Applications on SIMD Architectures},
  year       = {2014},
  issue_date = {June 2014},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {11},
  number     = {2},
  issn       = {1544-3566},
  url        = {https://doi.org/10.1145/2632215},
  doi        = {10.1145/2632215},
  abstract   = {Fine-grained data parallelism is increasingly common in the form of longer vectors integrated with mainstream processors (SSE, AVX) and various GPU architectures. This article develops support for exploiting such data parallelism for a class of nonnumeric, nongraphic applications, which perform computations while traversing many independent, irregular data structures. We address this problem by developing several novel techniques. First, for code generation, we develop an intermediate language for specifying such traversals, followed by a runtime scheduler that maps traversals to various SIMD units. Second, we observe that good data locality is crucial to sustained performance from SIMD architectures, whereas many applications that operate on irregular data structures (e.g., trees and graphs) have poor data locality. To address this challenge, we develop a set of data layout optimizations that improve spatial locality for applications that traverse many irregular data structures. Unlike prior data layout optimizations, our approach incorporates a notion of both interthread and intrathread spatial reuse into data layout. Finally, we enable performance portability (i.e., the ability to automatically optimize applications for different architectures) by accurately modeling the impact of inter- and intrathread locality on program performance. As a consequence, our model can predict which data layout optimization to use on a wide variety of SIMD architectures.To demonstrate the efficacy of our approach and optimizations, we first show how they enable up to a 12X speedup on one SIMD architecture for a set of real-world applications. To demonstrate that our approach enables performance portability, we show how our model predicts the optimal layout for applications across a diverse set of three real-world SIMD architectures, which offers as much as 45% speedup over a suboptimal solution.},
  journal    = {ACM Trans. Archit. Code Optim.},
  month      = {jun},
  articleno  = {16},
  numpages   = {31},
  keywords   = {SIMD, fine-grained parallelism, Irregular data structure}
}

@inproceedings{FAST,
  author    = {Changkyu Kim and
               Jatin Chhugani and
               Nadathur Satish and
               Eric Sedlar and
               Anthony D. Nguyen and
               Tim Kaldewey and
               Victor W. Lee and
               Scott A. Brandt and
               Pradeep Dubey},
  editor    = {Ahmed K. Elmagarmid and
               Divyakant Agrawal},
  title     = {{FAST:} fast architecture sensitive tree search on modern CPUs and
               GPUs},
  booktitle = {Proceedings of the {ACM} {SIGMOD} International Conference on Management
               of Data, {SIGMOD} 2010, Indianapolis, Indiana, USA, June 6-10, 2010},
  pages     = {339--350},
  publisher = {{ACM}},
  year      = {2010},
  url       = {https://doi.org/10.1145/1807167.1807206},
  doi       = {10.1145/1807167.1807206},
  timestamp = {Thu, 11 Mar 2021 15:20:15 +0100},
  biburl    = {https://dblp.org/rec/conf/sigmod/KimCSSNKLBD10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{LookingGlass,
  doi       = {10.48550/ARXIV.1912.09536},
  url       = {https://arxiv.org/abs/1912.09536},
  author    = {Psallidas, Fotis and Zhu, Yiwen and Karlas, Bojan and Interlandi, Matteo and Floratou, Avrilia and Karanasos, Konstantinos and Wu, Wentao and Zhang, Ce and Krishnan, Subru and Curino, Carlo and Weimer, Markus},
  keywords  = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Data Science through the looking glass and what we found there},
  publisher = {arXiv},
  year      = {2019},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{LHCModel,
  doi       = {10.48550/ARXIV.2001.06033},
  url       = {https://arxiv.org/abs/2001.06033},
  author    = {Lalchand, Vidhi},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Extracting more from boosted decision trees: A high energy physics case study},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DecisionTreesOverview,
  author  = {Kotsiantis, Sotiris},
  year    = {2013},
  month   = {04},
  pages   = {1-23},
  title   = {Decision trees: A recent overview},
  isbn    = {0269-2821},
  journal = {Artificial Intelligence Review},
  doi     = {10.1007/s10462-011-9272-4}
}

@article{YahooSearch,
  author  = {Cossock, David and Zhang, Tong},
  journal = {IEEE Transactions on Information Theory},
  title   = {Statistical Analysis of Bayes Optimal Subset Ranking},
  year    = {2008},
  volume  = {54},
  number  = {11},
  pages   = {5140-5154},
  doi     = {10.1109/TIT.2008.929939}
}

@article{Finance,
  author  = {Delen, Dursun and Kuzey, Cemil and Uyar, Ali},
  year    = {2013},
  month   = {08},
  pages   = {3970–3983},
  title   = {Measuring firm performance using financial ratios: A decision tree approach},
  volume  = {40},
  journal = {Expert Systems with Applications},
  doi     = {10.1016/j.eswa.2013.01.012}
}

@article{Med1,
  author  = {Azar, Ahmad and El-Metwally, Shereen},
  year    = {2013},
  month   = {11},
  pages   = {2387-2403},
  title   = {Decision tree classifiers for automated medical diagnosis},
  volume  = {23},
  journal = {Neural Computing and Applications},
  doi     = {10.1007/s00521-012-1196-7}
}

@article{Med2,
  author  = {Soni, Jyoti and Ansari, Ujma and Sharma, Dipesh and Soni, Sunita},
  year    = {2011},
  month   = {03},
  pages   = {43-48},
  title   = {Predictive Data Mining for Medical Diagnosis: An Overview of Heart Disease Prediction},
  volume  = {17},
  journal = {International Journal of Computer Applications},
  doi     = {10.5120/2237-2860}
}

@article{Facebook,
  author     = {Jongsoo Park and
                Maxim Naumov and
                Protonu Basu and
                Summer Deng and
                Aravind Kalaiah and
                Daya Shanker Khudia and
                James Law and
                Parth Malani and
                Andrey Malevich and
                Nadathur Satish and
                Juan Miguel Pino and
                Martin Schatz and
                Alexander Sidorov and
                Viswanath Sivakumar and
                Andrew Tulloch and
                Xiaodong Wang and
                Yiming Wu and
                Hector Yuen and
                Utku Diril and
                Dmytro Dzhulgakov and
                Kim M. Hazelwood and
                Bill Jia and
                Yangqing Jia and
                Lin Qiao and
                Vijay Rao and
                Nadav Rotem and
                Sungjoo Yoo and
                Mikhail Smelyanskiy},
  title      = {Deep Learning Inference in Facebook Data Centers: Characterization,
                Performance Optimizations and Hardware Implications},
  journal    = {CoRR},
  volume     = {abs/1811.09886},
  year       = {2018},
  url        = {http://arxiv.org/abs/1811.09886},
  eprinttype = {arXiv},
  eprint     = {1811.09886},
  timestamp  = {Mon, 16 Aug 2021 13:30:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1811-09886.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{atlas_sc98,
  author    = {R. Clint Whaley and Jack Dongarra},
  title     = {Automatically Tuned Linear Algebra Software},
  booktitle = {SuperComputing 1998: High Performance Networking and Computing},
  year      = {1998}
}

@article{BLIS,
  author     = {Van Zee, Field G. and van de Geijn, Robert A.},
  title      = {BLIS: A Framework for Rapidly Instantiating BLAS Functionality},
  year       = {2015},
  issue_date = {June 2015},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {41},
  number     = {3},
  issn       = {0098-3500},
  url        = {https://doi.org/10.1145/2764454},
  doi        = {10.1145/2764454},
  abstract   = {The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS operations can be expressed and optimized in terms of very simple kernels. While others have had similar insights, BLIS reduces the necessary kernels to what we believe is the simplest set that still supports the high performance that the computational science community demands. Higher-level framework code is generalized and implemented in ISO C99 so that it can be reused and/or reparameterized for different operations (and different architectures) with little to no modification. Inserting high-performance kernels into the framework facilitates the immediate optimization of any BLAS-like operations which are cast in terms of these kernels, and thus the framework acts as a productivity multiplier. Users of BLAS-dependent applications are given a choice of using the traditional Fortran-77 BLAS interface, a generalized C interface, or any other higher level interface that builds upon this latter API. Preliminary performance of level-2 and level-3 operations is observed to be competitive with two mature open source libraries (OpenBLAS and ATLAS) as well as an established commercial product (Intel MKL).},
  journal    = {ACM Trans. Math. Softw.},
  month      = {jun},
  articleno  = {14},
  numpages   = {33},
  keywords   = {libraries, Linear algebra, high-performance, matrix, BLAS}
}

@article{SPIRAL,
  author  = {Puschel, M. and Moura, J.M.F. and Johnson, J.R. and Padua, D. and Veloso, M.M. and Singer, B.W. and Jianxin Xiong and Franchetti, F. and Gacic, A. and Voronenko, Y. and Chen, K. and Johnson, R.W. and Rizzolo, N.},
  journal = {Proceedings of the IEEE},
  title   = {SPIRAL: Code Generation for DSP Transforms},
  year    = {2005},
  volume  = {93},
  number  = {2},
  pages   = {232-275},
  doi     = {10.1109/JPROC.2004.840306}
}

@article{Jansson2014gpuRFAG,
  title   = {gpuRF and gpuERT: Efficient and Scalable GPU Algorithms for Decision Tree Ensembles},
  author  = {Karl Jansson and H{\aa}kan Sundell and Henrik Bostr{\"o}m},
  journal = {2014 IEEE International Parallel \& Distributed Processing Symposium Workshops},
  year    = {2014},
  pages   = {1612-1621}
}

@article{Nasridinov2013DecisionTC,
  title   = {Decision tree construction on GPU: ubiquitous parallel computing approach},
  author  = {Aziz Nasridinov and Yangsun Lee and Young-Ho Park},
  journal = {Computing},
  year    = {2013},
  volume  = {96},
  pages   = {403-413}
}

@inproceedings{TensorFlow,
  author    = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  title     = {TensorFlow: A System for Large-Scale Machine Learning},
  year      = {2016},
  isbn      = {9781931971331},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {265–283},
  numpages  = {19},
  location  = {Savannah, GA, USA},
  series    = {OSDI'16}
}

@inproceedings{FFTW,
  author    = {Frigo, Matteo},
  title     = {A Fast Fourier Transform Compiler},
  year      = {1999},
  isbn      = {1581130945},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/301618.301661},
  doi       = {10.1145/301618.301661},
  abstract  = {The FFTW library for computing the discrete Fourier transform (DFT) has gained a wide acceptance in both academia and industry, because it provides excellent performance on a variety of machines (even competitive with or faster than equivalent libraries supplied by vendors). In FFTW, most of the performance-critical code was generated automatically by a special-purpose compiler, called genfft, that outputs C code. Written in Objective Caml, genfft can produce DFT programs for any input length, and it can specialize the DFT program for the common case where the input data are real instead of complex. Unexpectedly, genfft "discovered" algorithms that were previously unknown, and it was able to reduce the arithmetic complexity of some other existing algorithms. This paper describes the internals of this special-purpose compiler in some detail, and it argues that a specialized compiler is a valuable tool.},
  booktitle = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation},
  pages     = {169–180},
  numpages  = {12},
  location  = {Atlanta, Georgia, USA},
  series    = {PLDI '99}
}

@misc{TensorComprehensions,
  doi       = {10.48550/ARXIV.1802.04730},
  url       = {https://arxiv.org/abs/1802.04730},
  author    = {Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William S. and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  keywords  = {Programming Languages (cs.PL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{SageMaker,
  title        = {The total cost of ownership of Amazon SageMaker},
  howpublished = {\url{https://pages.awscloud.com/rs/112-TZM-766/images/Amazon_SageMaker_TCO_uf.pdf}},
  year         = {2020}
}

@misc{MLBenchmarks,
  title        = {Intel Machine Learning Benchmarks},
  howpublished = {\url{https://github.com/IntelPython/scikit-learn_bench}},
  year         = {2020}
}

@misc{IntelVTune,
  title        = {Intel VTune Profile},
  howpublished = {\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.40d3ea}},
  year         = {2022}
}

@misc{XGBoostPerfPR,
  title        = {XGBoost Predict Improvement},
  howpublished = {\url{https://github.com/dmlc/xgboost/pull/6127}},
  year         = {2020}
}

@misc{Cortex,
  doi       = {10.48550/ARXIV.2011.01383},
  url       = {https://arxiv.org/abs/2011.01383},
  author    = {Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip B. and Mowry, Todd C.},
  keywords  = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences, D.3.4, 68N20},
  title     = {Cortex: A Compiler for Recursive Deep Learning Models},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{ATCCNNOnCPU,
  author    = {Yizhi Liu and Yao Wang and Ruofei Yu and Mu Li and Vin Sharma and Yida Wang},
  title     = {Optimizing {CNN} Model Inference on {CPUs}},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  year      = {2019},
  isbn      = {978-1-939133-03-8},
  address   = {Renton, WA},
  pages     = {1025--1040},
  url       = {https://www.usenix.org/conference/atc19/presentation/liu-yizhi},
  publisher = {USENIX Association},
  month     = jul
}

@misc{XGBoostWinners,
  title        = {XGBoost Machine Learning Challenge Winning Solutions},
  howpublished = {\url{https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions}},
  year         = {2022}
}

@misc{LGBMWinners,
  title        = {XGBoost Machine Learning Challenge Winning Solutions},
  howpublished = {\url{https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions}},
  year         = {2022}
}

@article{Daghaghi2021AcceleratingSD,
  title   = {Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization, Quantizations, Memory Optimizations, and More},
  author  = {Shabnam Daghaghi and Nicholas Meisburger and Mengnan Zhao and Yong Wu and Sameh Gobriel and Charlie Tai and Anshumali Shrivastava},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2103.10891}
}

@misc{AzureCloud,
  title        = {What is Azure Machine Learning?},
  howpublished = {\url{https://docs.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning}},
  year         = {2022}
}

@misc{AmazonSageMaker,
  title        = {Amazon SageMaker Neo},
  howpublished = {\url{https://aws.amazon.com/sagemaker/neo/}},
  year         = {2022}
}

@misc{AmazonDLR,
  title        = {Amazon Neo AI DLR},
  howpublished = {\url{https://github.com/neo-ai/neo-ai-dlr}},
  year         = {2022}
}

@inproceedings{ChenW21,
  author    = {Qi Chen and 
               Bing Zhao and 
               Haidong Wang and 
               Mingqin Li and 
               Chuanjie Liu and 
               Zengzhong Li and 
               Mao Yang and 
               Jingdong Wang},
  title     = {SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search},
  booktitle = {35th Conference on Neural Information Processing Systems (NeurIPS 2021)},
  year      = {2021}
}

@inproceedings{DiskANN,
  author    = {Jayaram Subramanya, Suhas and Devvrit, Fnu and Simhadri, Harsha Vardhan and Krishnawamy, Ravishankar and Kadekodi, Rohan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node},
  url       = {https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{FBAppliedML,
  author    = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
  booktitle = {2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  title     = {Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {620-629},
  doi       = {10.1109/HPCA.2018.00059}
}

@misc{FBTrees,
  title        = {Evaluating Boosted Decision Trees for Billions of Users},
  howpublished = {\url{https://engineering.fb.com/2017/03/27/ml-applications/evaluating-boosted-decision-trees-for-billions-of-users/}},
  year         = {2017}
}

@inproceedings{Treebeard,
  author    = {Prasad, Ashwin and Rajendra, Sampath and Rajan, Kaushik and Govindarajan, R and Bondhugula, Uday},
  booktitle = {2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  title     = {Treebeard: An Optimizing Compiler for Decision Tree Based ML Inference},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {494-511},
  keywords  = {Codes;Microarchitecture;Optimizing compilers;Computational modeling;Machine learning;Benchmark testing;Hardware;optimizing Compiler;Decision Tree Ensemble;Decision Tree Inference;Vectorization;Machine Learning},
  doi       = {10.1109/MICRO56248.2022.00043}
}

@article{GoldenAge,
  author     = {Hennessy, John L. and Patterson, David A.},
  title      = {A new golden age for computer architecture},
  year       = {2019},
  issue_date = {February 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {62},
  number     = {2},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3282307},
  doi        = {10.1145/3282307},
  abstract   = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
  journal    = {Commun. ACM},
  month      = {jan},
  pages      = {48–60},
  numpages   = {13}
}

@article{DLNotAllYouNeed,
  author     = {Shwartz-Ziv, Ravid and Armon, Amitai},
  title      = {Tabular data: Deep learning is not all you need},
  year       = {2022},
  issue_date = {May 2022},
  publisher  = {Elsevier Science Publishers B. V.},
  address    = {NLD},
  volume     = {81},
  number     = {C},
  issn       = {1566-2535},
  url        = {https://doi.org/10.1016/j.inffus.2021.11.011},
  doi        = {10.1016/j.inffus.2021.11.011},
  journal    = {Inf. Fusion},
  month      = {may},
  pages      = {84–90},
  numpages   = {7},
  keywords   = {Hyperparameter optimization, Tree-based models, Deep neural networks, Tabular data}
}

@misc{TreebasedOutperformDL,
  title         = {Why do tree-based models still outperform deep learning on tabular data?},
  author        = {Léo Grinsztajn and Edouard Oyallon and Gaël Varoquaux},
  year          = {2022},
  eprint        = {2207.08815},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}