@book{lamport94,
  author    = {Leslie Lamport},
  title     = {{\LaTeX: A Document Preparation System}},
  year      = {1994},
  publisher = {Addison-Wesley},
  edition   = {2nd},
  address   = {Reading, Massachusetts}
}

@inproceedings{LLVM,
  author    = {Lattner, C. and Adve, V.},
  booktitle = {International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
  title     = {LLVM: a compilation framework for lifelong program analysis  amp; transformation},
  year      = {2004},
  volume    = {},
  number    = {},
  pages     = {75-86},
  doi       = {10.1109/CGO.2004.1281665}
}

@inproceedings{MLIR,
  author    = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle = {2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  title     = {MLIR: Scaling Compiler Infrastructure for Domain Specific Computation},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {2-14},
  doi       = {10.1109/CGO51591.2021.9370308}
}

@inproceedings{XGBoost,
  author    = {Chen, Tianqi and Guestrin, Carlos},
  title     = {XGBoost: A Scalable Tree Boosting System},
  year      = {2016},
  isbn      = {9781450342322},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2939672.2939785},
  doi       = {10.1145/2939672.2939785},
  abstract  = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {785–794},
  numpages  = {10},
  keywords  = {large-scale machine learning},
  location  = {San Francisco, California, USA},
  series    = {KDD '16}
}

@inproceedings{LightGBM,
  author    = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  title     = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {3149–3157},
  numpages  = {9},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@inproceedings{TVM,
  author    = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
  title     = {{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning},
  booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  year      = {2018},
  isbn      = {978-1-939133-08-3},
  address   = {Carlsbad, CA},
  pages     = {578--594},
  url       = {https://www.usenix.org/conference/osdi18/presentation/chen},
  publisher = {USENIX Association},
  month     = oct
}

@inproceedings{Tiramisu,
  author    = {Baghdadi, Riyadh and Ray, Jessica and Romdhane, Malek Ben and Del Sozzo, Emanuele and Akkas, Abdurrahman and Zhang, Yunming and Suriana, Patricia and Kamil, Shoaib and Amarasinghe, Saman},
  title     = {Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code},
  year      = {2019},
  isbn      = {9781728114361},
  publisher = {IEEE Press},
  abstract  = {This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.},
  booktitle = {Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization},
  pages     = {193–205},
  numpages  = {13},
  keywords  = {Code Optimization, Distributed Systems, Code Generation, Polyhedral Model, Deep Learning, Tensors, GPU},
  location  = {Washington, DC, USA},
  series    = {CGO 2019}
}

@misc{XLA,
  key    = {XLA},
  author = {Google Inc},
  title  = {{XLA (Accelerated Linear Algebra) for TensorFlow}},
  note   = {https://www.tensorflow.org/performance/xla/},
  year   = 2017
}

@misc{KaggleSurvey,
  title        = {Kaggle State of Data Science and Machine Learning 2021},
  howpublished = {\url{https://www.kaggle.com/kaggle-survey-2021}},
  note         = {Accessed: 2022-04-16}
}

@misc{Treelite,
  title        = {Treelite : model compiler for decision tree ensembles},
  howpublished = {\url{https://treelite.readthedocs.io/en/latest/}},
  note         = {Accessed: 2022-04-16}
}

@misc{Sklearn,
  title        = {scikit-learn : Machine Learning in Python},
  howpublished = {\url{https://scikit-learn.org/stable/}},
  note         = {Accessed: 2022-04-16}
}

@misc{CUTLASS,
  title        = {NVIDIA CUTLASS},
  howpublished = {\url{https://github.com/NVIDIA/cutlass}},
  note         = {Accessed: 2022-04-16}
}

@inproceedings{Halide,
  author    = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}do and Amarasinghe, Saman},
  title     = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
  year      = {2013},
  isbn      = {9781450320146},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2491956.2462176},
  doi       = {10.1145/2491956.2462176},
  abstract  = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
  booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {519–530},
  numpages  = {12},
  keywords  = {optimization, domain specific language, vectorization, parallelism, locality, image processing, compiler, autotuning, redundant computation, gpu},
  location  = {Seattle, Washington, USA},
  series    = {PLDI '13}
}

@article{VPred,
  author  = {Asadi, Nima and Lin, Jimmy and de Vries, Arjen P.},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  title   = {Runtime Optimizations for Tree-Based Machine Learning Models},
  year    = {2014},
  volume  = {26},
  number  = {9},
  pages   = {2281-2292},
  doi     = {10.1109/TKDE.2013.73}
}

@inproceedings{QuickScorer,
  author    = {Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Tonellotto, Nicola and Venturini, Rossano},
  title     = {QuickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees},
  year      = {2015},
  isbn      = {9781450336215},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2766462.2767733},
  doi       = {10.1145/2766462.2767733},
  abstract  = {Learning-to-Rank models based on additive ensembles of regression trees have proven to be very effective for ranking query results returned by Web search engines, a scenario where quality and efficiency requirements are very demanding. Unfortunately, the computational cost of these ranking models is high. Thus, several works already proposed solutions aiming at improving the efficiency of the scoring process by dealing with features and peculiarities of modern CPUs and memory hierarchies. In this paper, we present QuickScorer, a new algorithm that adopts a novel bitvector representation of the tree-based ranking model, and performs an interleaved traversal of the ensemble by means of simple logical bitwise operations. The performance of the proposed algorithm are unprecedented, due to its cache-aware approach, both in terms of data layout and access patterns, and to a control flow that entails very low branch mis-prediction rates. The experiments on real Learning-to-Rank datasets show that QuickScorer is able to achieve speedups over the best state-of-the-art baseline ranging from 2x to 6.5x.},
  booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {73–82},
  numpages  = {10},
  keywords  = {cache-aware algorithms, learning to rank, efficiency},
  location  = {Santiago, Chile},
  series    = {SIGIR '15}
}

@inproceedings{QuickScorer1,
  author    = {Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Tonellotto, Nicola and Venturini, Rossano},
  title     = {Exploiting CPU SIMD Extensions to Speed-up Document Scoring with Tree Ensembles},
  year      = {2016},
  isbn      = {9781450340694},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2911451.2914758},
  doi       = {10.1145/2911451.2914758},
  abstract  = {Scoring documents with learning-to-rank (LtR) models based on large ensembles of regression trees is currently deemed one of the best solutions to effectively rank query results to be returned by large scale Information Retrieval systems. This paper investigates the opportunities given by SIMD capabilities of modern CPUs to the end of efficiently evaluating regression trees ensembles. We propose V-QuickScorer (vQS), which exploits SIMD extensions to vectorize the document scoring, i.e., to perform the ensemble traversal by evaluating multiple documents simultaneously. We provide a comprehensive evaluation of vQS against the state of the art on three publicly available datasets. Experiments show that vQS provides speed-ups up to a factor of 3.2x.},
  booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {833–836},
  numpages  = {4},
  keywords  = {ensemble methods, learning to rank, document scoring},
  location  = {Pisa, Italy},
  series    = {SIGIR '16}
}

@inproceedings{CacheConscious1,
  author    = {Tang, Xun and Jin, Xin and Yang, Tao},
  title     = {Cache-Conscious Runtime Optimization for Ranking Ensembles},
  year      = {2014},
  isbn      = {9781450322577},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2600428.2609525},
  doi       = {10.1145/2600428.2609525},
  abstract  = {Multi-tree ensemble models have been proven to be effective for document ranking. Using a large number of trees can improve accuracy, but it takes time to calculate ranking scores of matched documents. This paper investigates data traversal methods for fast score calculation with a large ensemble. We propose a 2D blocking scheme for better cache utilization with simpler code structure compared to previous work. The experiments with several benchmarks show significant acceleration in score calculation without loss of ranking accuracy.},
  booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research amp; Development in Information Retrieval},
  pages     = {1123–1126},
  numpages  = {4},
  keywords  = {query processing, ensemble methods, cache locality},
  location  = {Gold Coast, Queensland, Australia},
  series    = {SIGIR '14}
}

@inproceedings{CacheConscious2,
  author    = {Jin, Xin and Yang, Tao and Tang, Xun},
  title     = {A Comparison of Cache Blocking Methods for Fast Execution of Ensemble-Based Score Computation},
  year      = {2016},
  isbn      = {9781450340694},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2911451.2911520},
  doi       = {10.1145/2911451.2911520},
  abstract  = {Machine-learned classification and ranking techniques often use ensembles to aggregate partial scores of feature vectors for high accuracy and the runtime score computation can become expensive when employing a large number of ensembles. The previous work has shown the judicious use of memory hierarchy in a modern CPU architecture which can effectively shorten the time of score computation. However, different traversal methods and blocking parameter settings can exhibit different cache and cost behavior depending on data and architectural characteristics. It is very time-consuming to conduct exhaustive search for performance comparison and optimum selection. This paper provides an analytic comparison of cache blocking methods on their data access performance with an approximation and proposes a fast guided sampling scheme to select a traversal method and blocking parameters for effective use of memory hierarchy. The evaluation studies with three datasets show that within a reasonable amount of time, the proposed scheme can identify a highly competitive solution that significantly accelerates score calculation.},
  booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {629–638},
  numpages  = {10},
  keywords  = {ensemble methods, cache locality, query processing},
  location  = {Pisa, Italy},
  series    = {SIGIR '16}
}

@inproceedings{Hummingbird,
  author    = {Supun Nakandala and Karla Saur and Gyeong-In Yu and Konstantinos Karanasos and Carlo Curino and Markus Weimer and Matteo Interlandi},
  title     = {A Tensor Compiler for Unified Machine Learning Prediction Serving},
  booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  year      = {2020},
  isbn      = {978-1-939133-19-9},
  pages     = {899--917},
  url       = {https://www.usenix.org/conference/osdi20/presentation/nakandala},
  publisher = {USENIX Association},
  month     = nov
}

@inproceedings{ProbBasedLayout,
  author    = {Buschjager, Sebastian and Chen, Kuan-Hsun and Chen, Jian-Jia and Morik, Katharina},
  booktitle = {2018 IEEE International Conference on Data Mining (ICDM)},
  title     = {Realization of Random Forest for Real-Time Evaluation through Tree Framing},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {19-28},
  doi       = {10.1109/ICDM.2018.00017}
}

@inproceedings{Tahoe,
  author    = {Xie, Zhen and Dong, Wenqian and Liu, Jiawen and Liu, Hang and Li, Dong},
  title     = {Tahoe: Tree Structure-Aware High Performance Inference Engine for Decision Tree Ensemble on GPU},
  year      = {2021},
  isbn      = {9781450383349},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3447786.3456251},
  doi       = {10.1145/3447786.3456251},
  abstract  = {Decision trees are widely used and often assembled as a forest to boost prediction accuracy. However, using decision trees for inference on GPU is challenging, because of irregular memory access patterns and imbalance workloads across threads. This paper proposes Tahoe, a tree structure-aware high performance inference engine for decision tree ensemble. Tahoe rearranges tree nodes to enable efficient and coalesced memory accesses; Tahoe also rearranges trees, such that trees with similar structures are grouped together in memory and assigned to threads in a balanced way. Besides memory access efficiency, we introduce a set of inference strategies, each of which uses shared memory differently and has different implications on reduction overhead. We introduce performance models to guide the selection of the inference strategies for arbitrary forests and data set. Tahoe consistently outperforms the state-of-the-art industry-quality library FIL by 3.82x, 2.59x, and 2.75x on three generations of NVIDIA GPUs (Kepler, Pascal, and Volta), respectively.},
  booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
  pages     = {426–440},
  numpages  = {15},
  keywords  = {tree structure, performance model, decision tree ensemble, decision tree inference},
  location  = {Online Event, United Kingdom},
  series    = {EuroSys '21}
}

@inproceedings{MilindTreeVectorization,
  author    = {Jo, Youngjoon and Goldfarb, Michael and Kulkarni, Milind},
  title     = {Automatic Vectorization of Tree Traversals},
  year      = {2013},
  isbn      = {9781479910212},
  publisher = {IEEE Press},
  abstract  = {Repeated tree traversals are ubiquitous in many domains such as scientific simulation, data mining and graphics. Modern commodity processors support SIMD instructions, and using these instructions to process multiple traversals at once has the potential to provide substantial performance improvements. Unfortunately these algorithms often feature highly diverging traversals which inhibit efficient SIMD utilization, to the point that other, less profitable sources of vectorization must be exploited instead. Previous work has proposed traversal splicing, a locality transformation for tree traversals, which dynamically reorders traversals based on previous behavior, based on the insight that traversals which have behaved similarly so far are likely to behave similarly in the future. In this work, we cast this dynamic reordering as a scheduling for efficient SIMD execution, and show that it can dramatically improve the SIMD utilization of diverging traversals, close to ideal utilization. For five irregular tree traversal algorithms, our techniques are able to deliver speedups of 2.78 on average over baseline implementations. Furthermore our techniques can effectively SIMDize algorithms that prior, manual vectorization attempts could not.},
  booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
  pages     = {363–374},
  numpages  = {12},
  keywords  = {SIMD, tree traversals, automatic vectorization, irregular programs},
  location  = {Edinburgh, Scotland, UK},
  series    = {PACT '13}
}

@article{PortableVM,
  author     = {Ren, Bin and Mytkowicz, Todd and Agrawal, Gagan},
  title      = {A Portable Optimization Engine for Accelerating Irregular Data-Traversal Applications on SIMD Architectures},
  year       = {2014},
  issue_date = {June 2014},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {11},
  number     = {2},
  issn       = {1544-3566},
  url        = {https://doi.org/10.1145/2632215},
  doi        = {10.1145/2632215},
  abstract   = {Fine-grained data parallelism is increasingly common in the form of longer vectors integrated with mainstream processors (SSE, AVX) and various GPU architectures. This article develops support for exploiting such data parallelism for a class of nonnumeric, nongraphic applications, which perform computations while traversing many independent, irregular data structures. We address this problem by developing several novel techniques. First, for code generation, we develop an intermediate language for specifying such traversals, followed by a runtime scheduler that maps traversals to various SIMD units. Second, we observe that good data locality is crucial to sustained performance from SIMD architectures, whereas many applications that operate on irregular data structures (e.g., trees and graphs) have poor data locality. To address this challenge, we develop a set of data layout optimizations that improve spatial locality for applications that traverse many irregular data structures. Unlike prior data layout optimizations, our approach incorporates a notion of both interthread and intrathread spatial reuse into data layout. Finally, we enable performance portability (i.e., the ability to automatically optimize applications for different architectures) by accurately modeling the impact of inter- and intrathread locality on program performance. As a consequence, our model can predict which data layout optimization to use on a wide variety of SIMD architectures.To demonstrate the efficacy of our approach and optimizations, we first show how they enable up to a 12X speedup on one SIMD architecture for a set of real-world applications. To demonstrate that our approach enables performance portability, we show how our model predicts the optimal layout for applications across a diverse set of three real-world SIMD architectures, which offers as much as 45% speedup over a suboptimal solution.},
  journal    = {ACM Trans. Archit. Code Optim.},
  month      = {jun},
  articleno  = {16},
  numpages   = {31},
  keywords   = {SIMD, fine-grained parallelism, Irregular data structure}
}

@inproceedings{FAST,
  author    = {Changkyu Kim and
               Jatin Chhugani and
               Nadathur Satish and
               Eric Sedlar and
               Anthony D. Nguyen and
               Tim Kaldewey and
               Victor W. Lee and
               Scott A. Brandt and
               Pradeep Dubey},
  editor    = {Ahmed K. Elmagarmid and
               Divyakant Agrawal},
  title     = {{FAST:} fast architecture sensitive tree search on modern CPUs and
               GPUs},
  booktitle = {Proceedings of the {ACM} {SIGMOD} International Conference on Management
               of Data, {SIGMOD} 2010, Indianapolis, Indiana, USA, June 6-10, 2010},
  pages     = {339--350},
  publisher = {{ACM}},
  year      = {2010},
  url       = {https://doi.org/10.1145/1807167.1807206},
  doi       = {10.1145/1807167.1807206},
  timestamp = {Thu, 11 Mar 2021 15:20:15 +0100},
  biburl    = {https://dblp.org/rec/conf/sigmod/KimCSSNKLBD10.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{LookingGlass,
  doi       = {10.48550/ARXIV.1912.09536},
  url       = {https://arxiv.org/abs/1912.09536},
  author    = {Psallidas, Fotis and Zhu, Yiwen and Karlas, Bojan and Interlandi, Matteo and Floratou, Avrilia and Karanasos, Konstantinos and Wu, Wentao and Zhang, Ce and Krishnan, Subru and Curino, Carlo and Weimer, Markus},
  keywords  = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Data Science through the looking glass and what we found there},
  publisher = {arXiv},
  year      = {2019},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{LHCModel,
  doi       = {10.48550/ARXIV.2001.06033},
  url       = {https://arxiv.org/abs/2001.06033},
  author    = {Lalchand, Vidhi},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Extracting more from boosted decision trees: A high energy physics case study},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DecisionTreesOverview,
  author  = {Kotsiantis, Sotiris},
  year    = {2013},
  month   = {04},
  pages   = {1-23},
  title   = {Decision trees: A recent overview},
  isbn    = {0269-2821},
  journal = {Artificial Intelligence Review},
  doi     = {10.1007/s10462-011-9272-4}
}

@article{YahooSearch,
  author  = {Cossock, David and Zhang, Tong},
  journal = {IEEE Transactions on Information Theory},
  title   = {Statistical Analysis of Bayes Optimal Subset Ranking},
  year    = {2008},
  volume  = {54},
  number  = {11},
  pages   = {5140-5154},
  doi     = {10.1109/TIT.2008.929939}
}

@article{Finance,
  author  = {Delen, Dursun and Kuzey, Cemil and Uyar, Ali},
  year    = {2013},
  month   = {08},
  pages   = {3970–3983},
  title   = {Measuring firm performance using financial ratios: A decision tree approach},
  volume  = {40},
  journal = {Expert Systems with Applications},
  doi     = {10.1016/j.eswa.2013.01.012}
}

@article{Med1,
  author  = {Azar, Ahmad and El-Metwally, Shereen},
  year    = {2013},
  month   = {11},
  pages   = {2387-2403},
  title   = {Decision tree classifiers for automated medical diagnosis},
  volume  = {23},
  journal = {Neural Computing and Applications},
  doi     = {10.1007/s00521-012-1196-7}
}

@article{Med2,
  author  = {Soni, Jyoti and Ansari, Ujma and Sharma, Dipesh and Soni, Sunita},
  year    = {2011},
  month   = {03},
  pages   = {43-48},
  title   = {Predictive Data Mining for Medical Diagnosis: An Overview of Heart Disease Prediction},
  volume  = {17},
  journal = {International Journal of Computer Applications},
  doi     = {10.5120/2237-2860}
}

@article{Facebook,
  author     = {Jongsoo Park and
                Maxim Naumov and
                Protonu Basu and
                Summer Deng and
                Aravind Kalaiah and
                Daya Shanker Khudia and
                James Law and
                Parth Malani and
                Andrey Malevich and
                Nadathur Satish and
                Juan Miguel Pino and
                Martin Schatz and
                Alexander Sidorov and
                Viswanath Sivakumar and
                Andrew Tulloch and
                Xiaodong Wang and
                Yiming Wu and
                Hector Yuen and
                Utku Diril and
                Dmytro Dzhulgakov and
                Kim M. Hazelwood and
                Bill Jia and
                Yangqing Jia and
                Lin Qiao and
                Vijay Rao and
                Nadav Rotem and
                Sungjoo Yoo and
                Mikhail Smelyanskiy},
  title      = {Deep Learning Inference in Facebook Data Centers: Characterization,
                Performance Optimizations and Hardware Implications},
  journal    = {CoRR},
  volume     = {abs/1811.09886},
  year       = {2018},
  url        = {http://arxiv.org/abs/1811.09886},
  eprinttype = {arXiv},
  eprint     = {1811.09886},
  timestamp  = {Mon, 16 Aug 2021 13:30:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1811-09886.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{atlas_sc98,
  author    = {R. Clint Whaley and Jack Dongarra},
  title     = {Automatically Tuned Linear Algebra Software},
  booktitle = {SuperComputing 1998: High Performance Networking and Computing},
  year      = {1998}
}

@article{BLIS,
  author     = {Van Zee, Field G. and van de Geijn, Robert A.},
  title      = {BLIS: A Framework for Rapidly Instantiating BLAS Functionality},
  year       = {2015},
  issue_date = {June 2015},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {41},
  number     = {3},
  issn       = {0098-3500},
  url        = {https://doi.org/10.1145/2764454},
  doi        = {10.1145/2764454},
  abstract   = {The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS operations can be expressed and optimized in terms of very simple kernels. While others have had similar insights, BLIS reduces the necessary kernels to what we believe is the simplest set that still supports the high performance that the computational science community demands. Higher-level framework code is generalized and implemented in ISO C99 so that it can be reused and/or reparameterized for different operations (and different architectures) with little to no modification. Inserting high-performance kernels into the framework facilitates the immediate optimization of any BLAS-like operations which are cast in terms of these kernels, and thus the framework acts as a productivity multiplier. Users of BLAS-dependent applications are given a choice of using the traditional Fortran-77 BLAS interface, a generalized C interface, or any other higher level interface that builds upon this latter API. Preliminary performance of level-2 and level-3 operations is observed to be competitive with two mature open source libraries (OpenBLAS and ATLAS) as well as an established commercial product (Intel MKL).},
  journal    = {ACM Trans. Math. Softw.},
  month      = {jun},
  articleno  = {14},
  numpages   = {33},
  keywords   = {libraries, Linear algebra, high-performance, matrix, BLAS}
}

@article{SPIRAL,
  author  = {Puschel, M. and Moura, J.M.F. and Johnson, J.R. and Padua, D. and Veloso, M.M. and Singer, B.W. and Jianxin Xiong and Franchetti, F. and Gacic, A. and Voronenko, Y. and Chen, K. and Johnson, R.W. and Rizzolo, N.},
  journal = {Proceedings of the IEEE},
  title   = {SPIRAL: Code Generation for DSP Transforms},
  year    = {2005},
  volume  = {93},
  number  = {2},
  pages   = {232-275},
  doi     = {10.1109/JPROC.2004.840306}
}

@article{Jansson2014gpuRFAG,
  title   = {gpuRF and gpuERT: Efficient and Scalable GPU Algorithms for Decision Tree Ensembles},
  author  = {Karl Jansson and H{\aa}kan Sundell and Henrik Bostr{\"o}m},
  journal = {2014 IEEE International Parallel \& Distributed Processing Symposium Workshops},
  year    = {2014},
  pages   = {1612-1621}
}

@article{Nasridinov2013DecisionTC,
  title   = {Decision tree construction on GPU: ubiquitous parallel computing approach},
  author  = {Aziz Nasridinov and Yangsun Lee and Young-Ho Park},
  journal = {Computing},
  year    = {2013},
  volume  = {96},
  pages   = {403-413}
}

@inproceedings{TensorFlow,
  author    = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  title     = {TensorFlow: A System for Large-Scale Machine Learning},
  year      = {2016},
  isbn      = {9781931971331},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {265–283},
  numpages  = {19},
  location  = {Savannah, GA, USA},
  series    = {OSDI'16}
}

@inproceedings{FFTW,
  author    = {Frigo, Matteo},
  title     = {A Fast Fourier Transform Compiler},
  year      = {1999},
  isbn      = {1581130945},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/301618.301661},
  doi       = {10.1145/301618.301661},
  abstract  = {The FFTW library for computing the discrete Fourier transform (DFT) has gained a wide acceptance in both academia and industry, because it provides excellent performance on a variety of machines (even competitive with or faster than equivalent libraries supplied by vendors). In FFTW, most of the performance-critical code was generated automatically by a special-purpose compiler, called genfft, that outputs C code. Written in Objective Caml, genfft can produce DFT programs for any input length, and it can specialize the DFT program for the common case where the input data are real instead of complex. Unexpectedly, genfft "discovered" algorithms that were previously unknown, and it was able to reduce the arithmetic complexity of some other existing algorithms. This paper describes the internals of this special-purpose compiler in some detail, and it argues that a specialized compiler is a valuable tool.},
  booktitle = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation},
  pages     = {169–180},
  numpages  = {12},
  location  = {Atlanta, Georgia, USA},
  series    = {PLDI '99}
}

@misc{TensorComprehensions,
  doi       = {10.48550/ARXIV.1802.04730},
  url       = {https://arxiv.org/abs/1802.04730},
  author    = {Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William S. and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  keywords  = {Programming Languages (cs.PL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{SageMaker,
  title        = {The total cost of ownership of Amazon SageMaker},
  howpublished = {\url{https://pages.awscloud.com/rs/112-TZM-766/images/Amazon_SageMaker_TCO_uf.pdf}},
  year         = {2020}
}

@misc{MLBenchmarks,
  title        = {Intel Machine Learning Benchmarks},
  howpublished = {\url{https://github.com/IntelPython/scikit-learn_bench}},
  year         = {2020}
}

@misc{IntelVTune,
  title        = {Intel VTune Profile},
  howpublished = {\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.40d3ea}},
  year         = {2022}
}

@misc{XGBoostPerfPR,
  title        = {XGBoost Predict Improvement},
  howpublished = {\url{https://github.com/dmlc/xgboost/pull/6127}},
  year         = {2020}
}

@misc{Cortex,
  doi       = {10.48550/ARXIV.2011.01383},
  url       = {https://arxiv.org/abs/2011.01383},
  author    = {Fegade, Pratik and Chen, Tianqi and Gibbons, Phillip B. and Mowry, Todd C.},
  keywords  = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences, D.3.4, 68N20},
  title     = {Cortex: A Compiler for Recursive Deep Learning Models},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{ATCCNNOnCPU,
  author    = {Yizhi Liu and Yao Wang and Ruofei Yu and Mu Li and Vin Sharma and Yida Wang},
  title     = {Optimizing {CNN} Model Inference on {CPUs}},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  year      = {2019},
  isbn      = {978-1-939133-03-8},
  address   = {Renton, WA},
  pages     = {1025--1040},
  url       = {https://www.usenix.org/conference/atc19/presentation/liu-yizhi},
  publisher = {USENIX Association},
  month     = jul
}

@misc{XGBoostWinners,
  title        = {XGBoost Machine Learning Challenge Winning Solutions},
  howpublished = {\url{https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions}},
  year         = {2022}
}

@misc{LGBMWinners,
  title        = {XGBoost Machine Learning Challenge Winning Solutions},
  howpublished = {\url{https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions}},
  year         = {2022}
}

@article{Daghaghi2021AcceleratingSD,
  title   = {Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization, Quantizations, Memory Optimizations, and More},
  author  = {Shabnam Daghaghi and Nicholas Meisburger and Mengnan Zhao and Yong Wu and Sameh Gobriel and Charlie Tai and Anshumali Shrivastava},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2103.10891}
}

@misc{AzureCloud,
  title        = {What is Azure Machine Learning?},
  howpublished = {\url{https://docs.microsoft.com/en-us/azure/machine-learning/overview-what-is-azure-machine-learning}},
  year         = {2022}
}

@misc{AmazonSageMaker,
  title        = {Amazon SageMaker Neo},
  howpublished = {\url{https://aws.amazon.com/sagemaker/neo/}},
  year         = {2022}
}

@misc{AmazonDLR,
  title        = {Amazon Neo AI DLR},
  howpublished = {\url{https://github.com/neo-ai/neo-ai-dlr}},
  year         = {2022}
}

@inproceedings{ChenW21,
  author    = {Qi Chen and 
               Bing Zhao and 
               Haidong Wang and 
               Mingqin Li and 
               Chuanjie Liu and 
               Zengzhong Li and 
               Mao Yang and 
               Jingdong Wang},
  title     = {SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search},
  booktitle = {35th Conference on Neural Information Processing Systems (NeurIPS 2021)},
  year      = {2021}
}

@inproceedings{DiskANN,
  author    = {Jayaram Subramanya, Suhas and Devvrit, Fnu and Simhadri, Harsha Vardhan and Krishnawamy, Ravishankar and Kadekodi, Rohan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node},
  url       = {https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{FBAppliedML,
  author    = {Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
  booktitle = {2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  title     = {Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {620-629},
  doi       = {10.1109/HPCA.2018.00059}
}

@misc{FBTrees,
  title        = {Evaluating Boosted Decision Trees for Billions of Users},
  howpublished = {\url{https://engineering.fb.com/2017/03/27/ml-applications/evaluating-boosted-decision-trees-for-billions-of-users/}},
  year         = {2017}
}

@inproceedings{Treebeard,
  author    = {Prasad, Ashwin and Rajendra, Sampath and Rajan, Kaushik and Govindarajan, R and Bondhugula, Uday},
  booktitle = {2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  title     = {Treebeard: An Optimizing Compiler for Decision Tree Based ML Inference},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {494-511},
  keywords  = {Codes;Microarchitecture;Optimizing compilers;Computational modeling;Machine learning;Benchmark testing;Hardware;optimizing Compiler;Decision Tree Ensemble;Decision Tree Inference;Vectorization;Machine Learning},
  doi       = {10.1109/MICRO56248.2022.00043}
}

@article{GoldenAge,
  author     = {Hennessy, John L. and Patterson, David A.},
  title      = {A new golden age for computer architecture},
  year       = {2019},
  issue_date = {February 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {62},
  number     = {2},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3282307},
  doi        = {10.1145/3282307},
  abstract   = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
  journal    = {Commun. ACM},
  month      = {jan},
  pages      = {48–60},
  numpages   = {13}
}

@article{DLNotAllYouNeed,
  author     = {Shwartz-Ziv, Ravid and Armon, Amitai},
  title      = {Tabular data: Deep learning is not all you need},
  year       = {2022},
  issue_date = {May 2022},
  publisher  = {Elsevier Science Publishers B. V.},
  address    = {NLD},
  volume     = {81},
  number     = {C},
  issn       = {1566-2535},
  url        = {https://doi.org/10.1016/j.inffus.2021.11.011},
  doi        = {10.1016/j.inffus.2021.11.011},
  journal    = {Inf. Fusion},
  month      = {may},
  pages      = {84–90},
  numpages   = {7},
  keywords   = {Hyperparameter optimization, Tree-based models, Deep neural networks, Tabular data}
}

@misc{TreebasedOutperformDL,
  title         = {Why do tree-based models still outperform deep learning on tabular data?},
  author        = {Léo Grinsztajn and Edouard Oyallon and Gaël Varoquaux},
  year          = {2022},
  eprint        = {2207.08815},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{TaoOfParallelism,
  author    = {Pingali, Keshav and Nguyen, Donald and Kulkarni, Milind and Burtscher, Martin and Hassaan, M. Amber and Kaleem, Rashid and Lee, Tsung-Hsien and Lenharth, Andrew and Manevich, Roman and M\'{e}ndez-Lojo, Mario and Prountzos, Dimitrios and Sui, Xin},
  title     = {The Tao of Parallelism in Algorithms},
  year      = {2011},
  isbn      = {9781450306638},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1993498.1993501},
  doi       = {10.1145/1993498.1993501},
  abstract  = {For more than thirty years, the parallel programming community has used the dependence graph as the main abstraction for reasoning about and exploiting parallelism in "regular" algorithms that use dense arrays, such as finite-differences and FFTs. In this paper, we argue that the dependence graph is not a suitable abstraction for algorithms in new application areas like machine learning and network analysis in which the key data structures are "irregular" data structures like graphs, trees, and sets.To address the need for better abstractions, we introduce a data-centric formulation of algorithms called the operator formulation in which an algorithm is expressed in terms of its action on data structures. This formulation is the basis for a structural analysis of algorithms that we call tao-analysis. Tao-analysis can be viewed as an abstraction of algorithms that distills out algorithmic properties important for parallelization. It reveals that a generalized form of data-parallelism called amorphous data-parallelism is ubiquitous in algorithms, and that, depending on the tao-structure of the algorithm, this parallelism may be exploited by compile-time, inspector-executor or optimistic parallelization, thereby unifying these seemingly unrelated parallelization techniques. Regular algorithms emerge as a special case of irregular algorithms, and many application-specific optimization techniques can be generalized to a broader context.These results suggest that the operator formulation and tao-analysis of algorithms can be the foundation of a systematic approach to parallel programming.},
  booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {12–25},
  numpages  = {14},
  keywords  = {operator formulation, irregular programs, galois system, tao-analysis, amorphous data-parallelism},
  location  = {San Jose, California, USA},
  series    = {PLDI '11}
}

@inproceedings{HybridCPUGPU,
  author    = {Liu, Jianqiao and Hegde, Nikhil and Kulkarni, Milind},
  title     = {Hybrid CPU-GPU Scheduling and Execution of Tree Traversals},
  year      = {2016},
  isbn      = {9781450343619},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2925426.2926261},
  doi       = {10.1145/2925426.2926261},
  abstract  = {GPUs offer the promise of massive, power-efficient parallelism. However, exploiting this parallelism requires code to be carefully structured to deal with the limitations of the SIMT execution model. In recent years, there has been much interest in mapping irregular applications to GPUs: applications with unpredictable, data-dependent behaviors. While most of the work in this space has focused on ad hoc implementations of specific algorithms, recent work has looked at generic techniques for mapping a large class of tree traversal algorithms to GPUs, through careful restructuring of the tree traversal algorithms to make them behave more regularly. Unfortunately, even this general approach for GPU execution of tree traversal algorithms is reliant on ad hoc, hand-written, algorithm-specific scheduling (i.e., assignment of threads to warps) to achieve high performance.The key challenge of scheduling is that it is a highly irregular process, that requires the inspection of thread behavior and then careful sorting of those threads into warps. In this paper, we present a novel scheduling and execution technique for tree traversal algorithms that is both general and automatic. The key novelty is a hybrid, inspector-executor approach: the GPU partially executes tasks to inspect thread behavior and transmits information back to the CPU, which uses that information to perform the scheduling itself, before executing the remaining, carefully scheduled, portion of the traversals on the GPU. We applied this framework to six tree traversal algorithms, achieving significant speedups over optimized GPU code that does not perform application-specific scheduling. Further, we show that in many cases, our hybrid approach is able to deliver better performance even than GPU code that uses handtuned, application-specific scheduling.},
  booktitle = {Proceedings of the 2016 International Conference on Supercomputing},
  articleno = {2},
  numpages  = {12},
  keywords  = {Irregular applications, GPU, Scheduling, Heterogeneous architectures, Tree traversal},
  location  = {Istanbul, Turkey},
  series    = {ICS '16}
}

@article{GraphIt,
  author     = {Zhang, Yunming and Yang, Mengjiao and Baghdadi, Riyadh and Kamil, Shoaib and Shun, Julian and Amarasinghe, Saman},
  title      = {GraphIt: a high-performance graph DSL},
  year       = {2018},
  issue_date = {November 2018},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {2},
  number     = {OOPSLA},
  url        = {https://doi.org/10.1145/3276491},
  doi        = {10.1145/3276491},
  abstract   = {The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. As a result, programmers must try different combinations of a large set of techniques, which make tradeoffs among locality, work-efficiency, and parallelism, to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks and domain specific languages (DSLs) lack flexibility, supporting only a limited set of optimizations. This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a separate scheduling language. The algorithm language simplifies expressing the algorithms, while exposing opportunities for optimizations. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together a large set of edge traversal, vertex data layout, and program structure optimizations. The separation of algorithm and schedule also enables us to build an autotuner on top of GraphIt to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. We evaluate GraphIt’s performance with seven algorithms on graphs with different structures and sizes. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8\texttimes{}, and is never more than 43\% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {oct},
  articleno  = {121},
  numpages   = {30},
  keywords   = {Big Data, Code Generation, Compiler Optimizations, Domain Specific Languages, Graph Algorithms, Parallel Programming Languages}
}

@inproceedings{GraphItGPU,
  author    = {Brahmakshatriya, Ajay and Zhang, Yunming and Hong, Changwan and Kamil, Shoaib and Shun, Julian and Amarasinghe, Saman},
  title     = {Compiling graph applications for GPUs with graphit},
  year      = {2021},
  isbn      = {9781728186139},
  publisher = {IEEE Press},
  url       = {https://doi.org/10.1109/CGO51591.2021.9370321},
  doi       = {10.1109/CGO51591.2021.9370321},
  abstract  = {The performance of graph programs depends highly on the algorithm, the size and structure of the input graphs, as well as the features of the underlying hardware. No single set of optimizations or one hardware platform works well across all settings. To achieve high performance, the programmer must carefully select which set of optimizations and hardware platforms to use. The GraphIt programming language makes it easy for the programmer to write the algorithm once and optimize it for different inputs using a scheduling language. However, GraphIt currently has no support for generating highperformance code for GPUs. Programmers must resort to re-implementing the entire algorithm from scratch in a low-level language with an entirely different set of abstractions and optimizations in order to achieve high performance on GPUs.We propose G2, an extension to the GraphIt compiler framework, that achieves high performance on both CPUs and GPUs using the same algorithm specification. G2 significantly expands the optimization space of GPU graph processing frameworks with a novel GPU scheduling language and compiler that enables combining load balancing, edge traversal direction, active vertexset creation, active vertexset processing ordering, and kernel fusion optimizations. G2 also introduces two performance optimizations, Edge-based Thread Warps CTAs load balancing (ETWC) and EdgeBlocking, to expand the optimization space for GPUs. ETWC improves load balancing by dynamically partitioning the edges of each vertex into blocks that are assigned to threads, warps, and CTAs for execution. EdgeBlocking improves the locality of the program by reordering the edges and restricting random memory accesses to fit within the L2 cache. We evaluate G2 on 5 algorithms and 9 input graphs on both Pascal and Volta generation NVIDIA GPUs, and show that it achieves up to 5.11\texttimes{} speedup over state-of-the-art GPU graph processing frameworks, and is the fastest on 66 out of the 90 experiments.},
  booktitle = {Proceedings of the 2021 IEEE/ACM International Symposium on Code Generation and Optimization},
  pages     = {248–261},
  numpages  = {14},
  keywords  = {graph processing, domain-specific languages, compiler optimizations, GPUs},
  location  = {Virtual Event, Republic of Korea},
  series    = {CGO '21}
}

@inproceedings{ChandanReduction,
  author    = {Reddy, Chandan and Kruse, Michael and Cohen, Albert},
  title     = {Reduction Drawing: Language Constructs and Polyhedral Compilation for Reductions on GPU},
  year      = {2016},
  isbn      = {9781450341219},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2967938.2967950},
  doi       = {10.1145/2967938.2967950},
  abstract  = {Reductions are common in scientific and data-crunching codes, and a typical source of bottlenecks on massively parallel architectures such as GPUs. Reductions are memory-bound, and achieving peak performance involves sophisticated optimizations. There exist libraries such as CUB and Thrust providing highly tuned implementations of reductions on GPUs. However, library APIs are not flexible enough to express user-defined reductions on arbitrary data types and array indexing schemes. Languages such as OpenACC provide declarative syntax to express reductions. Such approaches support a limited range of reduction operators and do not facilitate the application of complex program transformations in presence of reductions. We present language constructs that let a programmer express arbitrary reductions on user-defined data types matching the performance of tuned library implementations. We also extend a polyhedral compilation flow to process these user-defined reductions, enabling optimizations such as the fusion of multiple reductions, combining reductions with other loop transformations, and optimizing data transfers and storage in the presence of reductions. We implemented these language constructs and compilation methods in the PPCG framework and conducted experiments on multiple GPU targets. For single reductions the generated code performs on par with highly tuned libraries, and for multiple reductions it significantly outperforms both libraries and OpenACC on all platforms.},
  booktitle = {Proceedings of the 2016 International Conference on Parallel Architectures and Compilation},
  pages     = {87–97},
  numpages  = {11},
  keywords  = {polyhedral compilation, gpu optimizations, compiler transformations, automatic parallelization},
  location  = {Haifa, Israel},
  series    = {PACT '16}
}

@inproceedings{HalideReductions,
  author    = {Suriana, Patricia and Adams, Andrew and Kamil, Shoaib},
  title     = {Parallel associative reductions in halide},
  year      = {2017},
  isbn      = {9781509049318},
  publisher = {IEEE Press},
  abstract  = {Halide is a domain-specific language for fast image processing that separates pipelines into the algorithm, which defines what values are computed, and the schedule, which defines how they are computed. Changes to the schedule are guaranteed to not change the results. While Halide supports parallelizing and vectorizing naturally data-parallel operations, it does not support the same scheduling for reductions. Instead, the programmer must create data parallelism by manually factoring reductions into multiple stages. This manipulation of the algorithm can introduce bugs, impairs readability and portability, and makes it impossible for automatic scheduling methods to parallelize reductions.  We describe a new Halide scheduling primitive rfactor which moves this factoring transformation into the schedule, as well as a novel synthesis-based technique that takes serial Halide reductions and synthesizes an equivalent binary associative reduction operator and its identity. This enables us to automatically replace the original pipeline stage with a pair of stages which first compute partial results over slices of the reduction domain, and then combine them. Our technique permits parallelization and vectorization of Halide algorithms which previously required manipulating both the algorithm and schedule.},
  booktitle = {Proceedings of the 2017 International Symposium on Code Generation and Optimization},
  pages     = {281–291},
  numpages  = {11},
  location  = {Austin, USA},
  series    = {CGO '17}
}

@inproceedings{TangramReduction,
  author    = {Gonzalo, Simon Garcia De and Huang, Sitao and Gómez-Luna, Juan and Hammond, Simon and Mutlu, Onur and Hwu, Wen-mei},
  booktitle = {2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  title     = {Automatic Generation of Warp-Level Primitives and Atomic Instructions for Fast and Portable Parallel Reduction on GPUs},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {73-84},
  keywords  = {Graphics processing units;Programming;Computer architecture;Libraries;Hardware;DSL;Microarchitecture},
  doi       = {10.1109/CGO.2019.8661187}
}

@misc{FIL,
  title        = {RAPIDS Forest Inference Library: Prediction at 100 million rows per second},
  howpublished = {\url{https://medium.com/rapids-ai/rapids-forest-inference-library-prediction-at-100-million-rows-per-second-19558890bc35}},
  year         = {2019},
  note         = {Accessed: 2024-04-15}
}

@misc{RAPIDS,
  title        = {RAPIDS: GPU Accelerated Data Science},
  howpublished = {\url{https://rapids.ai/}},
  year         = {2024},
  note         = {Accessed: 2024-04-15}
}

@misc{XGBGPU,
  title        = {XGBoost GPU Support},
  howpublished = {\url{https://xgboost.readthedocs.io/en/stable/gpu/index.html}},
  year         = {2024},
  note         = {Accessed: 2024-04-15}
}

@misc{CUB,
  title        = {CUB: API Reference for CUB},
  howpublished = {\url{https://docs.nvidia.com/cuda/cub/index.html}},
  year         = {2024},
  note         = {Accessed: 2024-04-15}
}

@misc{Thrust,
  title        = {Thrust},
  howpublished = {\url{https://developer.nvidia.com/thrust}},
  year         = {2024},
  note         = {Accessed: 2024-04-15}
}

@inproceedings{Pencil,
  author    = {Baghdadi, Riyadh and Beaugnon, Ulysse and Cohen, Albert and Grosser, Tobias and Kruse, Michael and Reddy, Chandan and Verdoolaege, Sven and Betts, Adam and Donaldson, Alastair F. and Ketema, Jeroen and Absar, Javed and Van Haastregt, Sven and Kravets, Alexey and Lokhmotov, Anton and David, Robert and Hajiyev, Elnar},
  booktitle = {2015 International Conference on Parallel Architecture and Compilation (PACT)},
  title     = {PENCIL: A Platform-Neutral Compute Intermediate Language for Accelerator Programming},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {138-149},
  keywords  = {DSL;Optimization;Kernel;Image processing;Graphics processing units;Benchmark testing;Arrays;automatic optimization;intermediate language;polyhedral model;domain specific languages;OpenCL},
  doi       = {10.1109/PACT.2015.17}
}
