\section{Related Work}
\label{Sec:Related}
While several optimization strategies for decision tree based models have been 
studied in the literature, to the best of our knowledge, no systems that are 
capable of exploring the full optimization space exist. We describe related work 
and compare these systems to \Treebeard{} in this section.

\emph{Decision Tree Inference Systems:} 
Tahoe\cite{Tahoe} is a system that implements high-performance library routines and a 
performance model for tree inference on GPUs. Tahoe is a library-based system that picks 
between four predefined strategies to implement decision tree inference on GPUs.
In comparison, \Treebeard{} explores a much larger set of implementation options 
because it is a compiler. \Treebeard{} can also explore different in-memory 
representations for models. Also, \Treebeard{} generates code that is specific to 
a particular model, specializing both the parallelism (by deciding the thread block 
structure on a per model basis) and the kernel code itself by 
performing optimizations like tree walk unrolling and interleaving.
% In contrast, Tahoe 
% uses a library-based approach, and cannot generate code tailored to a model 
% like \Treebeard{} does.

RAPIDS FIL\cite{FIL} is a library that implements decision tree inference on GPUs
and is the most widely used production system for decision tree inference. While 
FIL does implement some heuristics to pick a good configuration for every model, 
these techniques are limited and the library essentially uses a single strategy 
and in-memory representation for all models. XGBoost \cite{XGBoost} also implements GPU 
support\cite{XGBGPU} but uses a single strategy and in-memory representation. 
% In contrast, \Treebeard{} is a compiler that can explore a much larger
% optimization space and can generate code that is tailored to a specific model.

% Finally, all of Tahoe's optimizations are designed to address GPU specific problems. 
% For example, it uses an elaborate and opaque heuristic based on locality sensitive hashing 
% to coalesce accesses to the GPU global memory. We believe that the tree tiling
% infrastructure described in this paper will allow us to deterministically 
% coalesce accesses to GPU global memory.
%As described previously, Hummingbird\cite{Hummingbird} uses tensor operations to perform decision tree inference. 
% While compiling decision tree inference to GPUs presents a distinct set of challenges,
% we believe, we can achieve this while reusing much of \Treebeard{}'s current infrastructure.
% Specifically, we expect that most of the HIR and MIR
% optimizations described in this paper will carry over directly while LIR 
% optimizations and in-memory representations will need to be retargeted.
% This can be the subject of a separate future work.
On CPUs, XGBoost\cite{XGBoost}, LightGBM\cite{LightGBM} and
scikit-learn\cite{Sklearn} are extremely popular. However, 
as mentioned in Section \ref{sec:intro}, none of these systems
provide portable performance across different target machines.
\TODO{Write about the PACT paper and whether our scheduling language can represent 
all the schedules they propose.}
Other systems that hide dependency stalls by interleaving tree walks\cite{VPred},
implement optimized algorithms for tree inference\cite{QuickScorer, QuickScorer1}
and improve cache performance of decision tree ensembles on CPUs\cite{CacheConscious1, CacheConscious2}
have been proposed in prior work. However, these systems are limited to CPUs.
Some systems have been proposed to parallelize decision tree training 
on CPUs and GPUs\cite{Jansson2014gpuRFAG, Nasridinov2013DecisionTC}.


\emph{Decision Tree Ensemble Compilers:}
Several compilers for decision tree ensembles have been proposed in the 
literature \cite{Treelite, Treebeard, Hummingbird}. \TreebeardOLD{} and Treelite
exclusively target CPUs and all their optimizations are designed purely for 
performance on CPUs. 
Treelite\cite{Treelite} is a model compiler that only  
generates \op{if-else} code for each tree in the model. 
% Its code generator is hard-coded 
% to expand trees in an ensemble into a series of \op{if-else} statements (one for 
% each node in a tree). Due to this, extending Treelite and reusing it to 
% target GPUs is not possible.

\TreebeardOLD{} is the work most closely related to \Treebeard{}. While we 
build on top of \TreebeardOLD{}, \Treebeard{} is a significant enhancement 
over \TreebeardOLD{}. 
% Most importantly, we re-architect \TreebeardOLD{} in 
% order to generate code for multiple target processors.
Specifically, we introduce the scheduling language and schedule exploration 
while also enhancing the IRs and support for parallelizing 
across trees through the implementation of a novel MLIR reduction dialect.
% It cannot perform any of the optimizations \Treebeard{} is designed to perform. 

Hummingbird\cite{Hummingbird} is a compiler that compiles traditional ML models
to tensor operations, thereby enabling them to be run on tensor-based frameworks like
TensorFlow\cite{TensorFlow}. Hummingbird can target both CPUs
and GPUs, but, as was shown earlier~\cite{Treebeard},
tensor operations are not the most efficient way to implement decision tree inference
and the performance of Hummingbird is significantly lower than 
that of other frameworks.

% Hummingbird\cite{Hummingbird} compiles traditional ML models to make use of tensor primitives so that
% they can be integrated into tensor-based frameworks like TensorFlow \cite{TensorFlow}.
% While Hummingbird does compile decision tree based models to both CPUs and GPUs, the primary aim of 
% the system is to leverage progress in tensor compilers by representing ML models as tensor operations. 
% Consequently, Hummingbird does not perform any optimizations tailored to decision trees. It 
% also does not perform model specific optimizations that are enabled by \Treebeard{}'s abstractions. 

% Asadi et. al.\cite{VPred} optimize tree walks by hiding dependency stalls 
% by interleaving tree walks. In contrast, \Treebeard{} is an extensible 
% optimizing compiler that carefully implements this and many other
% optimizations at different levels of abstraction.
% QuickScorer\cite{QuickScorer, QuickScorer1} is an algorithm that uses 
% bit manipulation to compute tree predictions. Even though QuickScorer is 
% extremely fast for smaller models, it does not scale well to larger 
% models\cite{ProbBasedLayout}. The goal of QuickScorer is orthogonal 
% to the goals of \Treebeard{} and the QuickScorer algorithm can easily  
% be integrated into \Treebeard{} as another traversal strategy for the 
% system to explore. Tang et. al.\cite{CacheConscious1} and Jin et. al.\cite{CacheConscious2}
% build models to predict cache performance of decision tree ensembles on CPUs.
% This work is again orthogonal to the work described in this paper. Some systems have been proposed 
% to parallelize decision tree training on CPUs and GPUs\cite{Jansson2014gpuRFAG, Nasridinov2013DecisionTC}.

%Two recently published systems explore 
%using GPUs for decision tree inference\cite{Tahoe, Hummingbird}.

\emph{Other Systems and Techniques:} Ren et. al.~\cite{PortableVM} design an
intermediate language and a virtual machine to enable vector execution of decision tree
inference. However, this virtual machine is itself implemented by 
hand on different target processors. This is clearly 
more expensive than \Treebeard{}'s approach.
% Additionally, even though they perform layout optimizations, their system does
% not perform any model specific optimizations.
Jo et. al.\cite{MilindTreeVectorization} describe code transformations and runtime 
techniques that help vectorize tree-based applications. However, they do not 
study optimizations specific to decision trees.
% Both these works vectorize tree walks by performing different tree walks on each
% vector lane. The main issue with this approach is the divergence of tree walks.
% Another issue is that memory accesses are \op{gather}'s rather than vector
% loads. \Treebeard{}'s approach to vectorization solves both these issues. Also,
% these approaches are not precluded by \Treebeard{}'s tiling based vectorization.
% Multiple tiled tree walks can be combined into a single vectorized walk. We
% leave an exploration of this to future work.  FAST\cite{FAST} is a system that
% accelerates tree structured index search on CPUs and GPUs. FAST defines a layout
% for the index tree that enables vectorization of the tree walk. FAST uses a tree
% tiling approach to vectorize tree walks. However, FAST only uses a single
% triangular tile shape. \Treebeard{}'s basic tiling algorithm is a generalization
% of the tiling used in FAST. If given a perfectly balanced tree, the basic tiling
% algorithm would return exactly the tiling used by FAST.  Also, the tree walks in
% FAST are hand coded using intrinsics on the CPU and CUDA on the GPU and
% therefore need repeated effort to implement on each target. 
Inspector-executor systems \cite{TaoOfParallelism,HybridCPUGPU} have been 
developed to parallelize tree walks but are not a good fit 
for decision tree inference as the individual node predicates are 
simple and the overhead of an inspector-executor system would be prohibitive.

\emph{Code Generation Systems from Other Domains:}
Several optimizing compilers and code generation techniques have been developed 
for other domains. TVM\cite{TVM}, Tiramisu\cite{Tiramisu}, and Tensor 
Comprehensions\cite{TensorComprehensions} are optimizing compilers 
for DNNs that can target a variety of processors. Similarly, 
Halide\cite{Halide} is a DSL and compiler primarily designed for 
image processing applications. The concept of separating the computation 
from the schedule was pioneered by Halide and has since been adopted 
by several other systems~\cite{TVM,Tiramisu,GraphIt}. However, to 
the best of our knowledge, \Treebeard{} is the first system to design
a scheduling language for decision tree inference optimization
and to build a system capable of state-of-the-art performance 
across different processors.
% {\sc {Cortex}} transforms 
% recursive computations in DNNs to loop based computations and optimizes them.
% However, the model properties {\sc {Cortex}} assumes to optimize
% generated code do not hold in the context of decision tree inference. For example, 
% {\sc {Cortex}} assumes that control flow depends purely on data structure 
% connectivity. This is not the case with decision trees. Control flow is determined 
% by the input value and not by the structure of the tree. This makes the 
% problems addressed by \Treebeard{} very different from the ones handled
% by {\sc {Cortex}}.
Libraries that compose or generate optimized implementations  
for BLAS\cite{BLIS, atlas_sc98, CUTLASS} and signal processing\cite{FFTW, SPIRAL}
have also been developed.
% BLIS\cite{BLIS} and ATLAS\cite{atlas_sc98}
% are systems to instantiate high performance BLAS routines on multiple 
% target architectures. CUTLASS\cite{CUTLASS} provides building blocks 
% in the form of C++ templates to quickly instantiate high performance 
% BLAS functions on different GPU architectures. SPIRAL\cite{SPIRAL}
% is a domain specific compiler for signal processing applications
% that instantiates high performance routines for linear transformations 
% like FFTs and FIR filters. FFTW\cite{FFTW} is a fast fourier transform 
% compiler that generates high performance FFT routines by customizing 
% the routine based on FFT size and the target machine. 
% However, no such frameworks for decision tree ensembles exist currently.
% \Treebeard{} is a first step in this direction and unlocks several future 
% optimization opportunities.

\emph{Reductions:} CUB\cite{CUB} and Thrust\cite{Thrust} are libraries 
that implement high-performance parallel reductions on GPUs. While they 
provide highly-tuned implementations to perform large reductions, 
it is not possible to fuse these functions with other computations  
as required in \Treebeard{}. Reddy et. al.~\cite{ChandanReduction}
describe language constructs in {\sc {Pencil}}~\cite{Pencil}
to express reductions and to represent and optimize them using the polyhedral 
framework. It is not clear how these techniques can be fused with 
other computations in arbitrary loop nests as required in \Treebeard{}.
Additionally, their system does not express the hierarchical nature of 
reductions and also only targets GPUs. Suriana et. al.~\cite{HalideReductions}
extend Halide to add support for factoring reductions in the Halide 
scheduling language and to synthesize reduction operators. De Gonzalo 
et. al.~\cite{TangramReduction} describe a system based on Tangram 
that composes several partial reduction implementations into different 
reduction implementations for GPUs and then searches through these
alternate implementations to find the best ones. In summary, none of 
these systems provide abstractions and a general framework to generate 
and optimize reductions across different target processors as \Treebeard{} does.