\section{Representing and Optimizing Reductions}
\label{sec:reduction}
\Treebeard{} needs to sum up individual tree predictions to compute 
the prediction of the model while performing inference. However,
generating fused reductions within arbitrary loop nests specified 
using \Treebeard{}'s scheduling language is non-trivial. We found 
that existing reduction support in MLIR is insufficient to code
generate and optimize these reductions. MLIR only supports reductions
of value types and does not provide ways to lower reductions to GPUs. 
To address this gap, we design an MLIR dialect that allows us to
specify accumulating values into an element of a multi-dimensional array
and can be lowered to CPU or GPU. 

The main abstraction we introduce is the \op{reduce} op. It models 
atomically accumulating values into an element of a 
multi-dimensional array (represented by an MLIR \op{memref}).
Consider the following \Treebeard{} schedule.
In our example, \op{N\_t} is the number of trees 
and \op{batch\_size} is the batch size. The schedule tiles the 
tree loop and parallelizes the resulting outer loop.
\begin{lstlisting}[style=c++]
  tile(tree, t0, t1, N_t/2);
  reorder(t0, t1, batch);
  parallel(t0);
\end{lstlisting}

The MIR generated by \Treebeard{} for the above schedule is as follows. 
\begin{lstlisting}[style=c++]
  float result[batch_size]
  model = ensemble(...) 
  par.for t0 = 0 to N_t step N_t/2:
    for t1 = 0 to N_t/2:
      for batch = 0 to batch_size:
        t = getTree(model, t0 + t1) 
        p = walkDecisionTree(t, rows[batch])
        reduce(result[batch], p)
\end{lstlisting}

The compiler simply generates a \op{reduce} op to perform
the required parallel reduction. 
The semantics of the \op{reduce} op is exactly the semantics of 
an atomic accumulation, i.e. it guarantees that all accumulations 
are correctly performed even in the presence of parallel loops. 
The \op{reduce} op is defined for all associative and commutative
reduction operations with a well-defined initial value. The 
reduction operator and the initial value are attributes
on the \op{reduce} op. 

Having modeled the reductions with an abstract operation, the 
aim now is to lower this to a correct and optimized 
implementation on both CPU and GPU. In order to do this, we 
first determine if any parallel loop iterations can accumulate 
into the same array element. We call such loops 
\emph{\textbf{reduction loops}}. If such loops exist, we 
\textbf{\emph{privatize}} the array for each iteration of the
loop. We call this process \textbf{\emph{legalization}}.
Subsequently, each privatized dimension 
can be reduced at the end of the reduction loop it was inserted for. 
%\TODO{We cannot do better than this in terms of memory usage} \TODO{Need a proof}.

In our example, \Treebeard{} determines that the \op{t0} loop is a reduction 
loop w.r.t the \op{result} array and therefore legalizes 
the reduction by inserting a privatized array 
\op{partResults}. The privatized dimension of this array 
is reduced at the end of the \op{t0} loop.

\begin{lstlisting}[style=c++]
  float result[batch_size]
  float partResults[2][batch_size]
  model = ensemble(...) 
  par.for t0 = 0 to N_t step N_t/2:
    for t1 = 0 to N_t/2:
      for batch = 0 to batch_size:
        t = getTree(model, t0 + t1) 
        p = walkDecisionTree(t, rows[batch])
        reduce(partResults[t0/(N_t/2)][batch], p)
  
  results = reduceDimension(partResults[:, :], 0)
\end{lstlisting}

The op \op{reduceDimension} reduces values across the specified
dimension of an n-dimensional array. Here, 
it reduces all elements of the first dimension (dimension 0). 
and produces a result memref with a single dimension of size \op{batch\_size}.

To reduce the amount of memory used by arrays introduced for reduction,
we introduce the \op{reduceDimInplace} operation. It is similar to the 
\op{reduceDimension} op except that it updates the input array inplace
rather than writing results to a target array. It writes results to the 
zeroth index of the dimension being reduced. We use this op to 
compute intermediate results when several reduction loops are identified.

% \begin{definition}
%  \textbf{\op{reduce\_dimension\_inplace(memref, dim, [indices], [rangeStart], [rangeEnd])}}:
%   Computes the reduction over the dimension specified by \op{dimension} and stores the 
%   result at index 0 of that dimension. \op{[indices]} must be a vector of \op{dim} elements
%    (or empty if the dimension being reduced is the first dimension). \op{[rangeStart]} 
%    and \op{[rangeEnd]} must have the same number of elements. If both are \op{null} (not passed), 
%    all elements of the corresponding dimension are reduced. 
  
%   The computation performed by the op is defined by the following equation.
  
%   $memref[\vec{\boldsymbol{indices}}, 0, \vec{\boldsymbol{k}}] = \sum_{i=0}^{shape[dim]} memref[\vec{\boldsymbol{indices}}, i, \vec{\boldsymbol{k}}]\quad   \forall \vec{\boldsymbol{k}} \in \left[[rangeStart_0, rangeEnd_0), ... , [rangeStart_n, rangeEnd_n)\right]$  
% \end{definition}

\subsection{Lowering Reduction Operations}
% We implement lowering of the operations defined above to both the CPU and GPU.
% Since the lowering pipeline from MIR to LIR are different for CPU and GPU 
% compilation, we implement lowering and optimization of our reduction dialect to CPUs and
% GPUs simply using different MLIR rewrite patterns. 
% In this section, we briefly describe 
% how we lower the reduction operations to the CPU and GPU. 

For both CPU and GPU, \op{reduce} is lowered to a sequence 
of load, compute and write operations. This is possible because 
legalization ensures that parallel threads do not write to the same 
array element. 
%\subsubsection{Lowering to CPU}

For CPUs, we lower \op{reduceDimInplace} and 
\op{reduceDimension} to a simple loop nest that goes over the specified
subset of the input array, performs the reduction and writes 
the result into the appropriate location of the target array. 
If the schedule specifies that the reduction is to be vectorized,
the lowering passes generate vector (LLVM IR) instructions for 
the reduction.
% then as many elements as specified by the vector width are read 
% from the input array as a vector, accumulated as a vector, and 
% finally written back to the target array.

% In general, this works 
% well because reductions are typically being performed on dimensions
% other than the inner-most dimension and therefore, this strategy
% loads successive elements from memory maximizing memory bandwidth 
% utilization. 

% \TODO{explain atomic reduction}

% \subsubsection{Lowering to GPU}
The same reduction abstractions can be lowered to efficient GPU implementations
and therefore, simplify higher-level code generation. 
% The lowering for 
% the inplace and non-inplace operations are essentially the same, except 
% for the target array and we do not distinguish between them except 
% for finally storing the result. 
%
% The lowering of the \op{reduceDim*} 
\Treebeard{} can lower these ops to either exploit
parallelism across the independent reductions or 
the inherent parallelism in the reduction by performing a divide and conquer 
reduction depending on whether there are enough independent reductions to keep all threads
in a thread block busy.
% then the lowering pass can generate code that performs one (or 
% multiple) reductions in each thread. If, however, there are not 
% enough independent reductions, then the lowering pass generates a tree 
% style reduction where multiple threads cooperate to perform a single reduction
% using inter-thread shuffles.
% Another feature specific to GPU reductions is the use of shared memory. 
Additionally, if the schedule specifies that the reduction needs to be performed 
using shared memory, the privatized buffer is allocated in shared memory. 
% The compiler only allocates as much shared memory 
% as needed to hold values processed by a single thread-block.
%  and 
% index offsets are appropriately rewritten to handle the differences between 
% the indexing of the target memref and the shared memory array.
Our abstractions for reductions allow our lowering passes to be 
agnostic of whether we use shared memory and therefore allow 
us to enable or disable shared memory use independently from the other 
parts of the compiler. 

% We find that our current implementation of lowering reduction operations 
% is sufficient for \Treebeard{} and reduction is not the bottleneck in the
% generated code. However, we believe this approach to enabling 
% higher level code generators to easily generate reductions 
% through simple abstractions and then having the compiler 
% automatically lower them to efficient implementation is an
% important area for future work with applicability in several 
% domains. 

%\TODO{Should we mention how we handle multi-class models?}
